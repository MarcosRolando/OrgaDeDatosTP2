{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwoork_agus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosRolando/OrgaDeDatosTP2/blob/main/NeuralNetwoork_agus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGS8tOq2uIJn"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import math as mt\r\n",
        "from tensorflow import feature_column\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y84PINJBAaX7"
      },
      "source": [
        "Tuneamos algunas cosas del DataFrame que notamos en el analisis de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxVjrxBo5slK"
      },
      "source": [
        "def preprocess_dataframe(df):\r\n",
        "\r\n",
        "  df = df[df['Stage'].isin(['Closed Won', 'Closed Lost'])]\r\n",
        "  df.loc[:, 'Stage'].replace({'Closed Won':1, 'Closed Lost':0}, inplace=True) #0 corresponde a que el caso fue Closed Lost, 1 a que fue Closed Won. Asi tenemos un problema de clasificacion binario que puede entender la red neuronal.\r\n",
        "\r\n",
        "  df.loc[:, 'Planned_Delivery_Start_Date'] = pd.to_datetime(df['Planned_Delivery_Start_Date'], 'coerce',\r\n",
        "                                                                  format='%m/%d/%Y')\r\n",
        "  df.loc[:, 'Planned_Delivery_End_Date'] = pd.to_datetime(df['Planned_Delivery_End_Date'], 'coerce',\r\n",
        "                                                                                      format='%m/%d/%Y')\r\n",
        "  df = df[df['Opportunity_ID'] != 9773] #Hardcodeo este filtrado porque el id 9773 tiene mal cargada la fecha de delviery end, dando una diferencia de 200 anios xd\"\r\n",
        "\r\n",
        "  #Pongo .loc porque pandas me jode con warnings que son falsos positivos de slice copy\"\r\n",
        "  #Gracias Pandas!\"\r\n",
        "\r\n",
        "  #Creamos una nueva columna (Feature Engineering) que contiene la longitud en dias \r\n",
        "  #estimada de la operacion. En el informe habiamos encontrado que aparentaba haber\r\n",
        "  #una relacion cuadratica de decrecimiento a medida que aumentaban los dias donde disminuia\r\n",
        "  #la chance de completar la operacion.\r\n",
        "  df['Delta_Time'] = df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']\r\n",
        "  df.loc[:, 'Delta_Time'] = df['Delta_Time'].dt.days\r\n",
        "\r\n",
        "  #Pasamos todo a dolares\r\n",
        "  currency_conversion = {'AUD':0.707612, 'EUR':1.131064, 'GBP':1.318055, 'JPY':0.008987, 'USD':1.0}\r\n",
        "  df['Total_Taxable_Amount_Currency'] = df[['Total_Taxable_Amount_Currency']].replace(currency_conversion)\r\n",
        "  df['Total_Taxable_Amount'] = df['Total_Taxable_Amount_Currency'] * df['Total_Taxable_Amount']\r\n",
        "\r\n",
        "  #Borro columnas que tengan el mismo dato en todas las entradas, o inconsecuentes como el ID / Opportunity_ID\r\n",
        "  #Algunas columnas borradas son porque pienso que no tienen incidencia, ir viendo.\r\n",
        "  #TODO: Analizar si el Sales_Contract_No no es que importe el numero en si, sino si tiene\r\n",
        "  #o no tiene numero de contrato. Por ahora no lo meto como input.\r\n",
        "  #TODO: Ver el mismo tema con la columna 'Price', la mayoria tiene None u Other\r\n",
        "  #y solo unos pocos tienen precio numerico. Quiza importe que tenga precio o no tenga,\r\n",
        "  #o si no tiene precio quiza importe si es None u Other. Por ahora no lo pongo\r\n",
        "  #como input.\r\n",
        "  df.drop(columns=['Submitted_for_Approval', 'Last_Activity', 'ASP_(converted)_Currency', \r\n",
        "                  'Prod_Category_A', 'ID', 'Opportunity_ID'],inplace=True)\r\n",
        "\r\n",
        "  #Renombramos las columnas que tienen caracteres que TensorFlow no acepta como validos.\r\n",
        "  #Estos particularmente son whitespace, coma y parentesis por ejemplo.\r\n",
        "  df.rename(columns={'ASP_(converted)':'ASP_converted','Pricing, Delivery_Terms_Quote_Appr':\r\n",
        "                    'Pricing_Delivery_Terms_Quote_Appr','Pricing, Delivery_Terms_Approved':\r\n",
        "                    'Pricing_Delivery_Terms_Approved','Source ':'Source'},inplace=True)\r\n",
        "\r\n",
        "  #Drop columnas que quiza podamos usar pero por ahora no las uso\r\n",
        "  df.drop(columns=['Account_Created_Date','Opportunity_Created_Date',\r\n",
        "                  'Quote_Expiry_Date','Last_Modified_Date',\r\n",
        "                  'Planned_Delivery_Start_Date','Planned_Delivery_End_Date',\r\n",
        "                  'Month','Delivery_Quarter', 'Delivery_Year', 'Actual_Delivery_Date',\r\n",
        "                  'Sales_Contract_No','Price','ASP','ASP_Currency','Total_Amount_Currency',\r\n",
        "                  'Total_Amount','Total_Taxable_Amount_Currency','Currency',\r\n",
        "                   'Product_Category_B','Last_Modified_By'],\r\n",
        "                   inplace=True)\r\n",
        "\r\n",
        "  #Definimos que tipo de feature es cada columna\r\n",
        "\r\n",
        "  #Debemos separar algunos de los registros para armar un set de test propio (no el de la catedra). De esta forma sabremos rapidamente\r\n",
        "  #si nuestro modelo esta dando resultados optimos o no sin necesidad de estar subiendo el TP a Kaggle constantemente.\r\n",
        "  #Sin embargo, no queremos usar tantos registros ya que estariamos disminuyendo el set de entrenamiento considerablemente.\r\n",
        "  #Podemos empezar reservando 2000 registros para el test de prueba y ver que onda. Pasariamos de tener 16 mil a 14 mil \r\n",
        "  #registros para el set de entrenamiento, no es una perdida importantisima creo en principio, asi que arrancamos con eso.\r\n",
        "\r\n",
        "  #Por otro lado, nuestro test de prueba deberia tener un 50 50 de Closed Won y Closed Lost, por lo que no podemos elegir asi nomas\r\n",
        "  #al azar.\r\n",
        "\r\n",
        "  #TODO: DEJO COMO TAREA ARMAR EL SET DE PRUEBA, POR AHORA SOLO NOS PREOCUPAMOS DE LOGRAR HACER ANDAR LA RED NEURONAL.\r\n",
        "  normalized_columns = ['ASP_converted','TRF','Total_Taxable_Amount']\r\n",
        "  for column in normalized_columns:\r\n",
        "    df[column] = (df[column] - df[column].mean()) / df[column].std()\r\n",
        "\r\n",
        "  df.fillna(value=0, inplace=True)\r\n",
        "\r\n",
        "  #Modifico la columna Brand para que en vez de decir que marca es, solo diga\r\n",
        "  #si tiene o no marca\r\n",
        "\r\n",
        "  df.loc[df['Brand'] == 'None', 'Brand'] = 'No'\r\n",
        "  df.loc[df['Brand'] != 'No', 'Brand'] = 'Yes'\r\n",
        "\r\n",
        "  #Pruebo volar duplicados, solo cambia el producto. Si el producto no importa\r\n",
        "  #entonces volar duplicados no deberia importar. Obviamente vuelo el producto en el que\r\n",
        "  #quede tambien.\r\n",
        "  #df.drop_duplicates('Opportunity_Name',inplace=True)\r\n",
        "  df.drop(columns=['Product_Name','Product_Family','Opportunity_Name'],inplace=True)\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjY1dvgAdSzw"
      },
      "source": [
        "# Metodo que pasa de DataFrame de Pandas a un DataSet de TensorFlow\r\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n",
        "  dataframe = dataframe.copy()\r\n",
        "  labels = dataframe.pop('Stage') #Retorna la columna Stage, eliminandolo simultaneamente del DataFrame. 'Stage' seria nuestra columna 'target', es decir, lo que queremos predecir.\r\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels)) #Crea un DataSet cuyos elementos son slices de los tensores pasados a la funcion. Ver documentacion oficial para comprender bien.\r\n",
        "  #En pocas palabras, le pasamos las columnas con los datos como diccionario (estilo 'columna':[dato1,dato2,dato3]) y una lista con los resultados estilo [resu1,resu2,resu3].\r\n",
        "  #Se genera un DataSet del estilo [('columna':[dato1,dato2,dato3], [resu1, resu2, resu3])].\r\n",
        "  #En realidad 'columna' es una lista de todas las columnas con sus correspondientes datos, pero se entiende la idea creo.\r\n",
        "  if shuffle:\r\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe)) #Al tener el buffer_size del mismo tamanio que la cantidad de datos del dataset, tenemos perfect shuffling (ver documentacion para comprender).\r\n",
        "    #Basicamente mezlcamos el dataset para que luego los batches que se armen contengan distintos elementos si lo entrenamos distintas veces.\r\n",
        "  ds = ds.batch(batch_size) #Arma batches de tamanio batch_size entre elementos consecutivos del DataSet.\r\n",
        "  return ds"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmaiJcl39e7X"
      },
      "source": [
        "#Arma las features, asignando las columnas del DataFrame segun corresponda al tipo de feature\r\n",
        "#(numerico, categorico, etc). Entiendase por feature a las columnas del DataFrame.\r\n",
        "def set_up_feature_columns(dataframe, numeric_columns, indicator_columns, bucket_columns):\r\n",
        "  features = []\r\n",
        "\r\n",
        "  #numeric features\r\n",
        "  for column_name in numeric_columns:\r\n",
        "    features.append(feature_column.numeric_column(column_name))\r\n",
        "\r\n",
        "  #bucket features\r\n",
        "  boundaries = [] #En principio este boundary es solo para la columna 'Delta Time'. Ver de como generalizar.\r\n",
        "  for i in range(38):\r\n",
        "    boundaries.append(i*5.0)\r\n",
        "\r\n",
        "  for column_name in bucket_columns:\r\n",
        "    range_column = feature_column.numeric_column(column_name)\r\n",
        "    bukect_column = feature_column.bucketized_column(range_column, boundaries)\r\n",
        "    features.append(bukect_column)\r\n",
        "\r\n",
        "  #indicator_columns (one-hot value vector, para aquellas columnas categoricas de pocas opciones)\r\n",
        "  for column_name in indicator_columns:\r\n",
        "    categorical_column = feature_column.categorical_column_with_vocabulary_list(\r\n",
        "                                          column_name, dataframe[column_name].unique())\r\n",
        "    indicator_column = feature_column.indicator_column(categorical_column)\r\n",
        "    features.append(indicator_column)\r\n",
        "\r\n",
        "  breaucretic_code_status = feature_column.\\\r\n",
        "                      crossed_column([feature_column.categorical_column_with_vocabulary_list(\"Bureaucratic_Code_0_Approval\", [0,1]),\\\r\n",
        "                                      feature_column.categorical_column_with_vocabulary_list(\"Bureaucratic_Code_0_Approved\", [0,1])], \\\r\n",
        "                                      hash_bucket_size=4)#Es 4 en vez de 3 por el caso de 01, lo cual deberia ser un error y no se\r\n",
        "                                                         #si esta en la base de datos, pero tal vez esta bueno dejar la posibilidad\r\n",
        "  features.append(feature_column.indicator_column(breaucretic_code_status))\r\n",
        "\r\n",
        "  return features"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0gHFhmD-AbE"
      },
      "source": [
        "Preparamos los features para el modelo, es decir, seteamos cada una de las columnas que vayamos a utilizar del DataFrame. Luego generamos el DataSet en base al DataFrame para darselo como input al modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bTkNirdLlCr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "4386e1bc-e0b0-477a-b8dc-8da827800198"
      },
      "source": [
        "  #Columnas que consideramos numericas\r\n",
        "  numeric_columns = ['Pricing_Delivery_Terms_Quote_Appr','Pricing_Delivery_Terms_Approved',\r\n",
        "                      'ASP_converted','TRF','Total_Taxable_Amount']\r\n",
        "\r\n",
        "  #Columnas que consideramos clasificatorias con rango numerico\r\n",
        "  bucket_columns = ['Delta_Time']\r\n",
        "\r\n",
        "  #Columnas que consideramos categoricas de pocos valores posibles\r\n",
        "  indicator_columns = ['Region','Bureaucratic_Code','Source',\r\n",
        "                        'Account_Owner','Account_Type',\r\n",
        "                        'Opportunity_Type','Quote_Type','Delivery_Terms',\r\n",
        "                        'Product_Type','Size','Territory','Billing_Country',\r\n",
        "                        'Account_Name','Opportunity_Owner',\r\n",
        "                        'Brand']#'Product_Category_B','Last_Modified_By'\r\n",
        "                        #'Product_Family', 'Product_Name', 'Opportunity_Name'\r\n",
        "\r\n",
        "  df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "  df = preprocess_dataframe(df)\r\n",
        "\r\n",
        "  features = set_up_feature_columns(df,numeric_columns,indicator_columns,bucket_columns)\r\n",
        "\r\n",
        "  #Separamos el DataFrame en uno de pruebo y el de entrenamiento. TODO: Ver el de validacion\r\n",
        "  df_test = df.head(200)\r\n",
        "  df.drop(df.head(200).index, inplace=True)\r\n",
        "  df_validation = df.tail(200)\r\n",
        "  df.drop(df.tail(200).index, inplace=True)\r\n",
        "\r\n",
        "  feature_layer = tf.keras.layers.DenseFeatures(features)\r\n",
        "  ds = df_to_dataset(df,batch_size=56, shuffle=False)\r\n",
        "  ds_test = df_to_dataset(df_test, batch_size=56, shuffle=False)\r\n",
        "  ds_validation = df_to_dataset(df_validation, batch_size=56, shuffle=False)\r\n",
        "\r\n",
        "  df.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4582: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>Territory</th>\n",
              "      <th>Pricing_Delivery_Terms_Quote_Appr</th>\n",
              "      <th>Pricing_Delivery_Terms_Approved</th>\n",
              "      <th>Bureaucratic_Code_0_Approval</th>\n",
              "      <th>Bureaucratic_Code_0_Approved</th>\n",
              "      <th>Bureaucratic_Code</th>\n",
              "      <th>Source</th>\n",
              "      <th>Billing_Country</th>\n",
              "      <th>Account_Name</th>\n",
              "      <th>Account_Owner</th>\n",
              "      <th>Opportunity_Owner</th>\n",
              "      <th>Account_Type</th>\n",
              "      <th>Opportunity_Type</th>\n",
              "      <th>Quote_Type</th>\n",
              "      <th>Delivery_Terms</th>\n",
              "      <th>Brand</th>\n",
              "      <th>Product_Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>ASP_converted</th>\n",
              "      <th>TRF</th>\n",
              "      <th>Total_Taxable_Amount</th>\n",
              "      <th>Stage</th>\n",
              "      <th>Delta_Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>Source_11</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Account_Name_1522</td>\n",
              "      <td>Person_Name_50</td>\n",
              "      <td>Person_Name_30</td>\n",
              "      <td>Account_Type_0</td>\n",
              "      <td>Opportunity_Type_7</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_4</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>-0.569434</td>\n",
              "      <td>-0.191691</td>\n",
              "      <td>-0.197911</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>Source_11</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Account_Name_1522</td>\n",
              "      <td>Person_Name_50</td>\n",
              "      <td>Person_Name_30</td>\n",
              "      <td>Account_Type_0</td>\n",
              "      <td>Opportunity_Type_7</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_4</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>-0.569434</td>\n",
              "      <td>-0.191691</td>\n",
              "      <td>-0.197911</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>Source_11</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Account_Name_1522</td>\n",
              "      <td>Person_Name_50</td>\n",
              "      <td>Person_Name_30</td>\n",
              "      <td>Account_Type_0</td>\n",
              "      <td>Opportunity_Type_7</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_4</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>-0.569434</td>\n",
              "      <td>-0.191691</td>\n",
              "      <td>-0.197911</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Japan</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>None</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Account_Name_1204</td>\n",
              "      <td>Person_Name_66</td>\n",
              "      <td>Person_Name_30</td>\n",
              "      <td>Account_Type_2</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_5</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0.293748</td>\n",
              "      <td>-0.109641</td>\n",
              "      <td>-0.132407</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Japan</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_1</td>\n",
              "      <td>None</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Account_Name_1204</td>\n",
              "      <td>Person_Name_66</td>\n",
              "      <td>Person_Name_66</td>\n",
              "      <td>Account_Type_2</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_5</td>\n",
              "      <td>No</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>0.293748</td>\n",
              "      <td>-0.191691</td>\n",
              "      <td>-0.184551</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Region Territory  ...  Stage  Delta_Time\n",
              "200  Japan     Japan  ...      1         4.0\n",
              "201  Japan     Japan  ...      1         4.0\n",
              "202  Japan     Japan  ...      1         4.0\n",
              "203  Japan     Japan  ...      1         7.0\n",
              "204  Japan     Japan  ...      0         7.0\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgLC9fhu-Mqu"
      },
      "source": [
        "Creamos y compilamos el modelo. En esta seccion se tunean las propiedades del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgXH6i3kL9ZN"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "  feature_layer,\r\n",
        "  #tf.keras.layers.Dropout(0.2),\r\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgHkvYWxP2h"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stA0Y5lI9-Jy"
      },
      "source": [
        "Entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6px_EtjoYRXq",
        "outputId": "cb977eab-edf9-4579-f683-7e9c18af7555"
      },
      "source": [
        "#Ignoren el WARNING, esta en la documentacion tambien. Nadie le da bola en StackOverflow xd.\r\n",
        "model.fit(ds, validation_data=ds_validation, epochs=60)\r\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Region': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=string>, 'Territory': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=string>, 'Pricing_Delivery_Terms_Quote_Appr': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=int64>, 'Pricing_Delivery_Terms_Approved': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approval': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approved': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=string>, 'Billing_Country': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=string>, 'Account_Name': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Account_Owner': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Opportunity_Owner': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Quote_Type': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Product_Type': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=string>, 'Size': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=string>, 'ASP_converted': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'TRF': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, 'Total_Taxable_Amount': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'Delta_Time': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Region': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=string>, 'Territory': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=string>, 'Pricing_Delivery_Terms_Quote_Appr': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=int64>, 'Pricing_Delivery_Terms_Approved': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approval': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approved': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=string>, 'Billing_Country': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=string>, 'Account_Name': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Account_Owner': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Opportunity_Owner': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Quote_Type': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Product_Type': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=string>, 'Size': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=string>, 'ASP_converted': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'TRF': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, 'Total_Taxable_Amount': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'Delta_Time': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "293/295 [============================>.] - ETA: 0s - loss: 0.6474 - accuracy: 0.6501WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Region': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=string>, 'Territory': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=string>, 'Pricing_Delivery_Terms_Quote_Appr': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=int64>, 'Pricing_Delivery_Terms_Approved': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approval': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approved': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=string>, 'Billing_Country': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=string>, 'Account_Name': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Account_Owner': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Opportunity_Owner': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Quote_Type': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Product_Type': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=string>, 'Size': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=string>, 'ASP_converted': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'TRF': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, 'Total_Taxable_Amount': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'Delta_Time': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "295/295 [==============================] - 4s 9ms/step - loss: 0.6471 - accuracy: 0.6506 - val_loss: 0.7495 - val_accuracy: 0.4700\n",
            "Epoch 2/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.5809 - accuracy: 0.6994 - val_loss: 0.7012 - val_accuracy: 0.5150\n",
            "Epoch 3/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.5410 - accuracy: 0.7179 - val_loss: 0.6569 - val_accuracy: 0.5400\n",
            "Epoch 4/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.5134 - accuracy: 0.7369 - val_loss: 0.6203 - val_accuracy: 0.5450\n",
            "Epoch 5/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.4928 - accuracy: 0.7505 - val_loss: 0.5910 - val_accuracy: 0.6400\n",
            "Epoch 6/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.4768 - accuracy: 0.7649 - val_loss: 0.5676 - val_accuracy: 0.6500\n",
            "Epoch 7/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.4640 - accuracy: 0.7814 - val_loss: 0.5489 - val_accuracy: 0.6850\n",
            "Epoch 8/60\n",
            "295/295 [==============================] - 2s 8ms/step - loss: 0.4535 - accuracy: 0.8002 - val_loss: 0.5337 - val_accuracy: 0.6850\n",
            "Epoch 9/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.4447 - accuracy: 0.8179 - val_loss: 0.5212 - val_accuracy: 0.6800\n",
            "Epoch 10/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.4372 - accuracy: 0.8186 - val_loss: 0.5109 - val_accuracy: 0.6800\n",
            "Epoch 11/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.4306 - accuracy: 0.8196 - val_loss: 0.5023 - val_accuracy: 0.6800\n",
            "Epoch 12/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.4249 - accuracy: 0.8207 - val_loss: 0.4950 - val_accuracy: 0.6800\n",
            "Epoch 13/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.4198 - accuracy: 0.8261 - val_loss: 0.4887 - val_accuracy: 0.6800\n",
            "Epoch 14/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.4152 - accuracy: 0.8280 - val_loss: 0.4833 - val_accuracy: 0.6800\n",
            "Epoch 15/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.4111 - accuracy: 0.8359 - val_loss: 0.4786 - val_accuracy: 0.7150\n",
            "Epoch 16/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.4073 - accuracy: 0.8469 - val_loss: 0.4744 - val_accuracy: 0.8700\n",
            "Epoch 17/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.4038 - accuracy: 0.8562 - val_loss: 0.4707 - val_accuracy: 0.8750\n",
            "Epoch 18/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.4006 - accuracy: 0.8574 - val_loss: 0.4673 - val_accuracy: 0.8750\n",
            "Epoch 19/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3976 - accuracy: 0.8589 - val_loss: 0.4643 - val_accuracy: 0.8750\n",
            "Epoch 20/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3948 - accuracy: 0.8604 - val_loss: 0.4615 - val_accuracy: 0.8750\n",
            "Epoch 21/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3921 - accuracy: 0.8614 - val_loss: 0.4590 - val_accuracy: 0.8750\n",
            "Epoch 22/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3897 - accuracy: 0.8624 - val_loss: 0.4567 - val_accuracy: 0.8850\n",
            "Epoch 23/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3873 - accuracy: 0.8625 - val_loss: 0.4545 - val_accuracy: 0.8850\n",
            "Epoch 24/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3851 - accuracy: 0.8636 - val_loss: 0.4525 - val_accuracy: 0.8800\n",
            "Epoch 25/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3830 - accuracy: 0.8634 - val_loss: 0.4506 - val_accuracy: 0.8800\n",
            "Epoch 26/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3810 - accuracy: 0.8643 - val_loss: 0.4488 - val_accuracy: 0.8800\n",
            "Epoch 27/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3791 - accuracy: 0.8656 - val_loss: 0.4471 - val_accuracy: 0.8800\n",
            "Epoch 28/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3772 - accuracy: 0.8668 - val_loss: 0.4455 - val_accuracy: 0.8800\n",
            "Epoch 29/60\n",
            "295/295 [==============================] - 2s 8ms/step - loss: 0.3755 - accuracy: 0.8673 - val_loss: 0.4440 - val_accuracy: 0.8800\n",
            "Epoch 30/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3738 - accuracy: 0.8682 - val_loss: 0.4425 - val_accuracy: 0.8800\n",
            "Epoch 31/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3721 - accuracy: 0.8686 - val_loss: 0.4412 - val_accuracy: 0.8750\n",
            "Epoch 32/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3706 - accuracy: 0.8695 - val_loss: 0.4398 - val_accuracy: 0.8750\n",
            "Epoch 33/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3690 - accuracy: 0.8696 - val_loss: 0.4386 - val_accuracy: 0.8750\n",
            "Epoch 34/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3676 - accuracy: 0.8704 - val_loss: 0.4373 - val_accuracy: 0.8750\n",
            "Epoch 35/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3662 - accuracy: 0.8716 - val_loss: 0.4362 - val_accuracy: 0.8750\n",
            "Epoch 36/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3648 - accuracy: 0.8718 - val_loss: 0.4350 - val_accuracy: 0.8750\n",
            "Epoch 37/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3635 - accuracy: 0.8721 - val_loss: 0.4339 - val_accuracy: 0.8750\n",
            "Epoch 38/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3622 - accuracy: 0.8723 - val_loss: 0.4329 - val_accuracy: 0.8800\n",
            "Epoch 39/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3609 - accuracy: 0.8721 - val_loss: 0.4319 - val_accuracy: 0.8800\n",
            "Epoch 40/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3597 - accuracy: 0.8721 - val_loss: 0.4309 - val_accuracy: 0.8800\n",
            "Epoch 41/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3585 - accuracy: 0.8724 - val_loss: 0.4299 - val_accuracy: 0.8750\n",
            "Epoch 42/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3573 - accuracy: 0.8727 - val_loss: 0.4290 - val_accuracy: 0.8750\n",
            "Epoch 43/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3562 - accuracy: 0.8732 - val_loss: 0.4281 - val_accuracy: 0.8750\n",
            "Epoch 44/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3551 - accuracy: 0.8740 - val_loss: 0.4272 - val_accuracy: 0.8750\n",
            "Epoch 45/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3541 - accuracy: 0.8738 - val_loss: 0.4264 - val_accuracy: 0.8750\n",
            "Epoch 46/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3530 - accuracy: 0.8743 - val_loss: 0.4256 - val_accuracy: 0.8750\n",
            "Epoch 47/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3520 - accuracy: 0.8746 - val_loss: 0.4248 - val_accuracy: 0.8750\n",
            "Epoch 48/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3510 - accuracy: 0.8748 - val_loss: 0.4240 - val_accuracy: 0.8750\n",
            "Epoch 49/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3501 - accuracy: 0.8749 - val_loss: 0.4233 - val_accuracy: 0.8750\n",
            "Epoch 50/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3491 - accuracy: 0.8751 - val_loss: 0.4226 - val_accuracy: 0.8750\n",
            "Epoch 51/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3482 - accuracy: 0.8752 - val_loss: 0.4218 - val_accuracy: 0.8750\n",
            "Epoch 52/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3473 - accuracy: 0.8750 - val_loss: 0.4212 - val_accuracy: 0.8750\n",
            "Epoch 53/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3464 - accuracy: 0.8751 - val_loss: 0.4205 - val_accuracy: 0.8750\n",
            "Epoch 54/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3456 - accuracy: 0.8755 - val_loss: 0.4198 - val_accuracy: 0.8750\n",
            "Epoch 55/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3447 - accuracy: 0.8761 - val_loss: 0.4192 - val_accuracy: 0.8750\n",
            "Epoch 56/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3439 - accuracy: 0.8763 - val_loss: 0.4186 - val_accuracy: 0.8750\n",
            "Epoch 57/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3431 - accuracy: 0.8763 - val_loss: 0.4180 - val_accuracy: 0.8750\n",
            "Epoch 58/60\n",
            "295/295 [==============================] - 2s 6ms/step - loss: 0.3423 - accuracy: 0.8762 - val_loss: 0.4174 - val_accuracy: 0.8750\n",
            "Epoch 59/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3416 - accuracy: 0.8767 - val_loss: 0.4169 - val_accuracy: 0.8750\n",
            "Epoch 60/60\n",
            "295/295 [==============================] - 2s 7ms/step - loss: 0.3408 - accuracy: 0.8769 - val_loss: 0.4163 - val_accuracy: 0.8750\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_features (DenseFeature multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  2009      \n",
            "=================================================================\n",
            "Total params: 2,009\n",
            "Trainable params: 2,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYTFqZSM-TA-"
      },
      "source": [
        "Evaluamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SDVpkRXMmLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d3a014-a832-4dbd-fd9a-5ef76a6d051f"
      },
      "source": [
        "model.evaluate(ds_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4226 - accuracy: 0.9350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4226423501968384, 0.9350000023841858]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1z7YhHrAGLc"
      },
      "source": [
        "Escribimos las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8BZJqotAFqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cfd6b6-6bfb-4343-a388-593a9a5162bc"
      },
      "source": [
        "frio_test_df = pd.read_csv('/content/Test_TP2_Datos_2020-2C.csv')\r\n",
        "frio_test_df['Stage'] = 'Closed Won' #Esto esta solo para que funque todo, no lo uso. No se bien como armarlo sin los labels de Stage. TODO: Averiguar como es!\r\n",
        "aux_df = frio_test_df[['Opportunity_ID']] #Esta columna la vuela el preprocesado sino\r\n",
        "frio_test_df = preprocess_dataframe(frio_test_df)\r\n",
        "frio_test_ds = df_to_dataset(frio_test_df, shuffle=False, batch_size=56)\r\n",
        "predictions = model.predict(frio_test_ds)\r\n",
        "\r\n",
        "#aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True) #Lo hacia el preprocesado pero es verdad que lo copie antes a este xd, perdon Agus, paja de dejarlo lindo.\r\n",
        "aux_df['Target'] = predictions\r\n",
        "\r\n",
        "aux_df['Target'] = aux_df.groupby(by='Opportunity_ID').transform(lambda x: x.mean())\r\n",
        "aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True)\r\n",
        "\r\n",
        "aux_df.to_csv('prediccionesFrioFrio.csv', index=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Region': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=string>, 'Territory': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=string>, 'Pricing_Delivery_Terms_Quote_Appr': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=int64>, 'Pricing_Delivery_Terms_Approved': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approval': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approved': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=string>, 'Billing_Country': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=string>, 'Account_Name': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Account_Owner': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Opportunity_Owner': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Quote_Type': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Product_Type': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=string>, 'Size': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=string>, 'ASP_converted': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'TRF': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=float64>, 'Total_Taxable_Amount': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'Delta_Time': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>}\n",
            "Consider rewriting this model with the Functional API.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}