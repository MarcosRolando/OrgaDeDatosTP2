{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvZAjB5a99jsUageqg8/kk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosRolando/OrgaDeDatosTP2/blob/main/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meR57jS5Iqjo",
        "outputId": "4ac0aa3c-f7d0-4ea3-e601-f65cb189e215"
      },
      "source": [
        "!pip install xgboost\r\n",
        "import xgboost as xgb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import math as mt\r\n",
        "import regex as re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLh00eNULRuQ"
      },
      "source": [
        "def preprocess_dataframe(df):\r\n",
        "\r\n",
        "  df.fillna(value=0, inplace=True) #Reemplazamos NAN por 0, ya que NAN rompe a Tensorflow\r\n",
        "\r\n",
        "  #Renombramos las columnas que tienen caracteres que TensorFlow no acepta como validos.\r\n",
        "  #Estos particularmente son whitespace, coma y parentesis por ejemplo.\r\n",
        "  df.rename(columns={'ASP_(converted)':'ASP_converted','Pricing, Delivery_Terms_Quote_Appr':\r\n",
        "                    'Pricing_Delivery_Terms_Quote_Appr','Pricing, Delivery_Terms_Approved':\r\n",
        "                    'Pricing_Delivery_Terms_Approved','Source ':'Source'},inplace=True)\r\n",
        "\r\n",
        "  df = df[df['Stage'].isin(['Closed Won', 'Closed Lost'])]\r\n",
        "  df.loc[:, 'Stage'].replace({'Closed Won':1, 'Closed Lost':0}, inplace=True) #0 corresponde a que el caso fue Closed Lost, 1 a que fue Closed Won. Asi tenemos un problema de clasificacion binario que puede entender la red neuronal.\r\n",
        "\r\n",
        "  df.loc[:, 'Planned_Delivery_Start_Date'] = pd.to_datetime(df['Planned_Delivery_Start_Date'], 'coerce',\r\n",
        "                                                                  format='%m/%d/%Y')\r\n",
        "  df.loc[:, 'Planned_Delivery_End_Date'] = pd.to_datetime(df['Planned_Delivery_End_Date'], 'coerce',\r\n",
        "                                                                                      format='%m/%d/%Y')\r\n",
        "  df.loc[:, 'Last_Modified_Date'] = pd.to_datetime(df['Last_Modified_Date'], 'coerce',\r\n",
        "                                                                                      format='%m/%d/%Y')\r\n",
        "  df.loc[:, 'Quote_Expiry_Date'] = pd.to_datetime(df['Quote_Expiry_Date'], 'coerce', format='%m/%d/%Y')\r\n",
        "\r\n",
        "  df.loc[:, 'Opportunity_Created_Date'] = pd.to_datetime(df['Opportunity_Created_Date'], 'coerce',\r\n",
        "                                                                                      format='%m/%d/%Y')\r\n",
        "  df.loc[:, 'Account_Created_Date'] = pd.to_datetime(df['Account_Created_Date'], 'coerce', format='%m/%d/%Y')\r\n",
        "\r\n",
        "  df = df[df['Opportunity_ID'] != 9773] #Hardcodeo este filtrado porque el id 9773 tiene mal cargada la fecha de delivery end, dando una diferencia de 200 anios xd\"\r\n",
        "\r\n",
        "  #Pongo .loc porque pandas me jode con warnings que son falsos positivos de slice copy\"\r\n",
        "  #Gracias Pandas!\"\r\n",
        "\r\n",
        "  #Creamos una nueva columna (Feature Engineering) que contiene la longitud en dias \r\n",
        "  #estimada de la operacion. En el informe habiamos encontrado que aparentaba haber\r\n",
        "  #una relacion cuadratica de decrecimiento a medida que aumentaban los dias donde disminuia\r\n",
        "  #la chance de completar la operacion.\r\n",
        "  df['Planned_Opportunity_Time'] = df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']\r\n",
        "  df.loc[:, 'Planned_Opportunity_Time'] = df['Planned_Opportunity_Time'].dt.days\r\n",
        "  df['Planned_Opportunity_Time'] = df['Planned_Opportunity_Time'].replace({np.nan:10.0}) #Reemplazo con 10 porque los que no tienen fecha final ganan el 60%, y el analisis de los datos da que el 60% es maso a los 10 dias. Asi no jodo el resto de los datos\r\n",
        "  df['Planned_Opportunity_Time'] = df.groupby('Opportunity_ID')['Planned_Opportunity_Time'].transform('max')\r\n",
        "\r\n",
        "  df['End_Date_vs_Modified_Date_Difference'] = df['Planned_Delivery_End_Date'] - df['Last_Modified_Date']\r\n",
        "  df.loc[:, 'End_Date_vs_Modified_Date_Difference'] = df['End_Date_vs_Modified_Date_Difference'].dt.days\r\n",
        "  df['End_Date_vs_Modified_Date_Difference'] = df['End_Date_vs_Modified_Date_Difference'].replace({np.nan:10.0})\r\n",
        "  df['End_Date_vs_Modified_Date_Difference'] = df.groupby('Opportunity_ID')['End_Date_vs_Modified_Date_Difference'].transform('max')\r\n",
        "\r\n",
        "  df['Expiry_Date_vs_Modified_Date_Difference'] = df['Planned_Delivery_End_Date'] - df['Quote_Expiry_Date']\r\n",
        "  df.loc[:, 'Expiry_Date_vs_Modified_Date_Difference'] = df['Expiry_Date_vs_Modified_Date_Difference'].dt.days\r\n",
        "  df['Expiry_Date_vs_Modified_Date_Difference'] = df['Expiry_Date_vs_Modified_Date_Difference'].replace({np.nan:10.0})\r\n",
        "  df['Expiry_Date_vs_Modified_Date_Difference'] = df.groupby('Opportunity_ID')['Expiry_Date_vs_Modified_Date_Difference'].transform('max')\r\n",
        "\r\n",
        "  df['Acc_vs_Opp_Creation_Dates'] = df['Opportunity_Created_Date'] - df['Account_Created_Date']\r\n",
        "  df.loc[:, 'Acc_vs_Opp_Creation_Dates'] = df['Acc_vs_Opp_Creation_Dates'].dt.days\r\n",
        "  df['Expiry_Date_vs_Modified_Date_Difference'] = df['Acc_vs_Opp_Creation_Dates'].replace({np.nan:10})\r\n",
        "  df['Acc_vs_Opp_Creation_Dates'] = df.groupby('Opportunity_ID')['Acc_vs_Opp_Creation_Dates'].transform('max')\r\n",
        "\r\n",
        "  #Agrego una columna que indique la cantidad de productos que tiene esa\r\n",
        "  #oportunidad\r\n",
        "  df['Product'] = 1\r\n",
        "  df['Product_Amount'] = df.groupby('Opportunity_ID')['Product'].transform(lambda x: x.sum())\r\n",
        "\r\n",
        "\r\n",
        "  #Pasamos todo a dolares\r\n",
        "  currency_conversion = {'AUD':0.707612, 'EUR':1.131064, 'GBP':1.318055, 'JPY':0.008987, 'USD':1.0}\r\n",
        "  df['Total_Taxable_Amount_Currency'] = df[['Total_Taxable_Amount_Currency']].replace(currency_conversion)\r\n",
        "  df['Total_Taxable_Amount'] = df['Total_Taxable_Amount_Currency'] * df['Total_Taxable_Amount']\r\n",
        "  #df['Total_Taxable_Amount'] = df.groupby(\"Opportunity_ID\")['Total_Taxable_Amount'].transform(\"sum\")\r\n",
        "  df['Total_Taxable_Amount'] = df['Total_Taxable_Amount'] * df['Product_Amount']\r\n",
        "\r\n",
        "  #Modifico la columna Brand para que en vez de decir que marca es, solo diga\r\n",
        "  #si tiene o no marca. Es importante aclarar que verificamos que siempre que una oportunidad\r\n",
        "  #tiene un producto con marca entonces todos sus productos tienen marca. Esto se cumple\r\n",
        "  #tanto en el set de entrenamiento como en el de test, por lo tanto al hacer drop_duplicates\r\n",
        "  #no nos va a pasar nunca el caso donde nos pudieramos quedar con una entrada de producto\r\n",
        "  #sin marca mientras que algun otro producto si tuviera, ya que confirmamos que o todos tienen\r\n",
        "  #marca o ninguno tiene.\r\n",
        "  df.loc[df['Brand'] == 'None', 'Brand'] = 'No'\r\n",
        "  df.loc[df['Brand'] != 'No', 'Brand'] = 'Yes'\r\n",
        "\r\n",
        "  #Agrego una columna que indica si el owner de la cuenta es el mismo que el de la oportunidad\r\n",
        "  #o no\r\n",
        "  df['Same_Owner'] = (df['Account_Owner'] == df['Opportunity_Owner'])\r\n",
        "  df['Same_Owner'] = df['Same_Owner'].replace({False:'No', True:'Yes'})\r\n",
        "\r\n",
        "  #Agrego columna que indica si el ultimo que modifico la oportunidad es el mismo que el opportunity owner\r\n",
        "  df['Same_Owner_Modifier'] = (df['Last_Modified_By'] == df['Opportunity_Owner'])\r\n",
        "  df['Same_Owner_Modifier'] = df['Same_Owner_Modifier'].replace({False:'No', True:'Yes'})\r\n",
        "\r\n",
        "  #Agrego una columna que indica si tiene o no fecha de expiracion\r\n",
        "  df['Quote_Expiry_Date'] = (df['Quote_Expiry_Date'] != 'NaT')\r\n",
        "  df.rename(columns={'Quote_Expiry_Date':'Has_Expiry_Date'}, inplace=True)\r\n",
        "  df['Has_Expiry_Date'] = df['Has_Expiry_Date'].replace({True:'Yes',False:'No'})\r\n",
        "\r\n",
        "  #Reemplazo las 4 columnas de aprobacion por solo 2 columnas que indiquen si tuvo la aprobacion\r\n",
        "  #de delivery y burocratica o no. Recalco que si nunca la necesito seria equivalente a si\r\n",
        "  #la necesito y la consiguio.\r\n",
        "  df['Delivery_Approved'] = df['Pricing_Delivery_Terms_Quote_Appr'] + df['Pricing_Delivery_Terms_Approved']\r\n",
        "  df['Delivery_Approved'] = df['Delivery_Approved'].replace({0:1, 1:0, 2:1})\r\n",
        "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_0_Approval'] + df['Bureaucratic_Code_0_Approved']\r\n",
        "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_Approved'].replace({0:1, 1:0, 2:1})\r\n",
        "  df['Approved'] = df['Delivery_Approved'] & df['Bureaucratic_Code_Approved']\r\n",
        "\r\n",
        "  #Cambio TRF por una columna que es el valor maximo de los TRF de la oportunidad\r\n",
        "  df[\"TRF\"] = df.groupby(\"Opportunity_ID\")[\"TRF\"].transform(\"max\")\r\n",
        "\r\n",
        "  def combineProducts(x):\r\n",
        "    products = \"\"\r\n",
        "    added = []\r\n",
        "    for product in x:\r\n",
        "      product = re.findall('\\d+', product)[0]\r\n",
        "      if added.count(product) == 0:\r\n",
        "        products += product\r\n",
        "        added.append(product)\r\n",
        "    return products\r\n",
        "\r\n",
        "  #Junto todos los productos en una sola entrada\r\n",
        "  df['Products'] = df.groupby('Opportunity_ID')['Product_Family'].transform(combineProducts)\r\n",
        "\r\n",
        "  #Pruebo volar duplicados, solo cambia el producto. Si el producto no importa\r\n",
        "  #entonces volar duplicados no deberia importar. Obviamente vuelo el producto en el que\r\n",
        "  #quede tambien.\r\n",
        "  df.drop_duplicates('Opportunity_Name',inplace=True)\r\n",
        "  df.drop(columns=['Product_Name','Product_Family','Opportunity_Name'],inplace=True)\r\n",
        "\r\n",
        "  #Normalizo las columnas numericas\r\n",
        "  normalized_columns = ['ASP_converted','TRF','Total_Taxable_Amount', 'Product_Amount',\r\n",
        "                        'Planned_Opportunity_Time', 'End_Date_vs_Modified_Date_Difference',\r\n",
        "                        'Expiry_Date_vs_Modified_Date_Difference', 'Acc_vs_Opp_Creation_Dates']\r\n",
        "\r\n",
        "  for column in normalized_columns:\r\n",
        "    df[column] = (df[column] - df[column].mean()) / df[column].std()\r\n",
        "\r\n",
        "  #Borro columnas que tengan el mismo dato en todas las entradas, o inconsecuentes como el ID / Opportunity_ID\r\n",
        "  #Algunas columnas borradas son porque pienso que no tienen incidencia, ir viendo.\r\n",
        "  #TODO: Analizar si el Sales_Contract_No no es que importe el numero en si, sino si tiene\r\n",
        "  #o no tiene numero de contrato. Por ahora no lo meto como input.\r\n",
        "  #TODO: Ver el mismo tema con la columna 'Price', la mayoria tiene None u Other\r\n",
        "  #y solo unos pocos tienen precio numerico. Quiza importe que tenga precio o no tenga,\r\n",
        "  #o si no tiene precio quiza importe si es None u Other. Por ahora no lo pongo\r\n",
        "  #como input.\r\n",
        "  df.drop(columns=['Submitted_for_Approval', 'Last_Activity', 'ASP_(converted)_Currency', \r\n",
        "                  'Prod_Category_A', 'ID', 'Opportunity_ID', \r\n",
        "                   'Actual_Delivery_Date'],inplace=True)\r\n",
        "\r\n",
        "  #Drop columnas que quiza podamos usar pero por ahora no las uso\r\n",
        "  df.drop(columns=['Account_Created_Date','Opportunity_Created_Date',\r\n",
        "                  'Last_Modified_Date',\r\n",
        "                  'Planned_Delivery_Start_Date','Planned_Delivery_End_Date',\r\n",
        "                  'Month',\r\n",
        "                  'Delivery_Year',\r\n",
        "                  'Price','ASP','Total_Amount_Currency',\r\n",
        "                  'Total_Amount','Total_Taxable_Amount_Currency', 'Currency',\r\n",
        "                   'Product_Category_B','Last_Modified_By', 'Account_Owner',\r\n",
        "                   'Opportunity_Owner','Account_Name','Product_Type',\r\n",
        "                   'Billing_Country', 'Sales_Contract_No',\r\n",
        "                   'Product', 'Products','Territory']\r\n",
        "                   ,inplace=True)\r\n",
        "\r\n",
        "\r\n",
        "  df = df[['Source', 'Opportunity_Type', 'Brand', 'Planned_Opportunity_Time','Total_Taxable_Amount',\r\n",
        "           'Product_Amount','ASP_converted','TRF', 'Bureaucratic_Code','Quote_Type',\r\n",
        "           'Expiry_Date_vs_Modified_Date_Difference', \"Stage\"]]\r\n",
        "\r\n",
        "  #Agrego esto para que se pueda usar XGBoost\r\n",
        "  onehot_columns = ['Source', 'Opportunity_Type', 'Brand', 'Bureaucratic_Code','Quote_Type']\r\n",
        "\r\n",
        "  onehot_df = df[onehot_columns]\r\n",
        "  onehot_df = pd.get_dummies(onehot_df, columns = onehot_columns)\r\n",
        "  df_no_one_hot = df.drop(onehot_columns, axis = 1)\r\n",
        "  df_one_hot = pd.concat([df_no_one_hot, onehot_df], axis = 1)\r\n",
        "\r\n",
        "  #Definimos que tipo de feature es cada columna\r\n",
        "\r\n",
        "  #Debemos separar algunos de los registros para armar un set de test propio (no el de la catedra). De esta forma sabremos rapidamente\r\n",
        "  #si nuestro modelo esta dando resultados optimos o no sin necesidad de estar subiendo el TP a Kaggle constantemente.\r\n",
        "  #Sin embargo, no queremos usar tantos registros ya que estariamos disminuyendo el set de entrenamiento considerablemente.\r\n",
        "  #Podemos empezar reservando 2000 registros para el test de prueba y ver que onda. Pasariamos de tener 16 mil a 14 mil \r\n",
        "  #registros para el set de entrenamiento, no es una perdida importantisima creo en principio, asi que arrancamos con eso.\r\n",
        "\r\n",
        "  #Por otro lado, nuestro test de prueba deberia tener un 50 50 de Closed Won y Closed Lost, por lo que no podemos elegir asi nomas\r\n",
        "  #al azar.\r\n",
        "\r\n",
        "  return df_one_hot"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CisZ6_WwNBVh"
      },
      "source": [
        "def log_loss(predictions, results):\r\n",
        "  acum = 0\r\n",
        "  if len(predictions) != len(results):\r\n",
        "    print(\"Predicciones y resultados de distinto tama√±o\")\r\n",
        "    return None\r\n",
        "  for i in range(len(predictions)):\r\n",
        "    if results[i] == 1:\r\n",
        "      acum += mt.log(predictions[i])\r\n",
        "    elif results[i] == 0:\r\n",
        "      acum += mt.log(1-predictions[i])\r\n",
        "    else:\r\n",
        "      print(\"Resultado inesperado\")\r\n",
        "      return None\r\n",
        "  return -acum/len(predictions)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WLVnmLtVIJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c9cbd8-98ad-4d21-81a9-51126219a3ff"
      },
      "source": [
        "# read in data\r\n",
        "#dtrain = xgb.DMatrix(\"/content/Train_TP2_Datos_2020-2C.csv?format=csv\")\r\n",
        "#dtest = xgb.DMatrix(\"/content/Test_TP2_Datos_2020-2C.csv?format=csv\")\r\n",
        "df_train = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "df_train = preprocess_dataframe(df_train)\r\n",
        "columns_order = df_train.columns\r\n",
        "test_lines = 200\r\n",
        "np.random.seed(931)\r\n",
        "drop_indices = np.random.choice(df_train.index, test_lines, replace=False)\r\n",
        "df_test = df_train.loc[drop_indices, :]\r\n",
        "df_train.drop(drop_indices, inplace=True)\r\n",
        "\r\n",
        "x_train = df_train.drop(\"Stage\", axis = 1)\r\n",
        "y_train = df_train[\"Stage\"]\r\n",
        "x_test = df_test.drop(\"Stage\", axis = 1)\r\n",
        "y_test = df_test[\"Stage\"]\r\n",
        "\r\n",
        "\"\"\"x_train = df_train.iloc[:,:-1]\r\n",
        "y_train = df_train.iloc[:,-1]\r\n",
        "x_test = df_test.iloc[:,:-1]\r\n",
        "y_test = df_test.iloc[:,-1]\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "# specify parameters via map\r\n",
        "#param = {\"max_depth\":4, \"eta\":0.15, \"subsample\":0.3, \"grow_policy\":\"lossguide\", \"objective\":\"binary:logistic\" }\r\n",
        "\"\"\"param = {\"max_depth\":4, \"eta\":0.15, \"subsample\":0.3, \"colsample_bylevel\":0.5, \"colsample_bytree\":1, \r\n",
        "         \"min_child_weight\":5, \"objective\":\"binary:logistic\" }\"\"\"\r\n",
        "param = {\"max_depth\":4, \"eta\":0.15, \"subsample\":0.6, \"colsample_bylevel\":0.75, \r\n",
        "         \"colsample_bytree\":0.5, \"objective\":\"binary:logistic\" }\r\n",
        "epochs = 500\r\n",
        "#epochs = 60\r\n",
        "#bst = xgb.train(param, xgb.DMatrix(data=x_train,label=y_train), epochs)\r\n",
        "\"\"\"bst = xgb.train(param, xgb.DMatrix(data=x_train,label=y_train), epochs, \r\n",
        "                early_stopping_rounds=100,\r\n",
        "                evals= [ (xgb.DMatrix(data=x_train,label=y_train),'train'),(xgb.DMatrix(data=x_test,label=y_test),'eval')])\"\"\"\r\n",
        "bst = xgb.train(param, xgb.DMatrix(data=x_train,label=y_train), epochs, \r\n",
        "                early_stopping_rounds=20,\r\n",
        "                evals= [(xgb.DMatrix(data=x_test,label=y_test),'eval')])\r\n",
        "# make prediction\r\n",
        "preds = bst.predict(xgb.DMatrix(data=x_test,label=y_test))\r\n",
        "\r\n",
        "print(\"Loss entrenamiento: \" + str(log_loss(bst.predict(xgb.DMatrix(data=x_train,label=y_train)), y_train.to_list())))\r\n",
        "print(\"Loss test: \" + str(log_loss(preds, y_test.to_list())))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4582: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\teval-error:0.25\n",
            "Will train until eval-error hasn't improved in 20 rounds.\n",
            "[1]\teval-error:0.23\n",
            "[2]\teval-error:0.21\n",
            "[3]\teval-error:0.195\n",
            "[4]\teval-error:0.205\n",
            "[5]\teval-error:0.185\n",
            "[6]\teval-error:0.195\n",
            "[7]\teval-error:0.185\n",
            "[8]\teval-error:0.19\n",
            "[9]\teval-error:0.195\n",
            "[10]\teval-error:0.195\n",
            "[11]\teval-error:0.2\n",
            "[12]\teval-error:0.2\n",
            "[13]\teval-error:0.195\n",
            "[14]\teval-error:0.195\n",
            "[15]\teval-error:0.195\n",
            "[16]\teval-error:0.195\n",
            "[17]\teval-error:0.2\n",
            "[18]\teval-error:0.205\n",
            "[19]\teval-error:0.195\n",
            "[20]\teval-error:0.19\n",
            "[21]\teval-error:0.2\n",
            "[22]\teval-error:0.19\n",
            "[23]\teval-error:0.19\n",
            "[24]\teval-error:0.195\n",
            "[25]\teval-error:0.2\n",
            "Stopping. Best iteration:\n",
            "[5]\teval-error:0.185\n",
            "\n",
            "Loss entrenamiento: 0.432451281043769\n",
            "Loss test: 0.4445458336318652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVeiiU48FpO8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "1a1ad633-66b9-483a-abf8-fbb813f38fe3"
      },
      "source": [
        "\r\n",
        "frio_test_df = pd.read_csv('/content/Test_TP2_Datos_2020-2C.csv')\r\n",
        "frio_test_df['Stage'] = 'Closed Won' #Esto esta solo para que funque todo, no lo uso. No se bien como armarlo sin los labels de Stage. TODO: Averiguar como es!\r\n",
        "aux_df = frio_test_df[['Opportunity_ID']] #Esta columna la vuela el preprocesado sino\r\n",
        "frio_test_df = preprocess_dataframe(frio_test_df)\r\n",
        "\r\n",
        "for col in columns_order:\r\n",
        "  if not col in frio_test_df.columns:\r\n",
        "    frio_test_df[col] = 0\r\n",
        "\r\n",
        "frio_test_df = frio_test_df[columns_order]\r\n",
        "x_real_test = frio_test_df.drop(\"Stage\", axis = 1)\r\n",
        "y_real_test = frio_test_df[\"Stage\"]\r\n",
        "\r\n",
        "m = xgb.DMatrix(data = x_real_test, label = y_real_test)\r\n",
        "predictions = bst.predict(m)\r\n",
        "\r\n",
        "aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True) #Lo hacia el preprocesado pero es verdad que lo copie antes a este xd, perdon Agus, paja de dejarlo lindo.\r\n",
        "aux_df['Target'] = predictions\r\n",
        "\r\n",
        "#aux_df['Target'] = aux_df.groupby(by='Opportunity_ID').transform(lambda x: x.mean())\r\n",
        "#aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True)\r\n",
        "\r\n",
        "aux_df.to_csv('prediccionesFrioFrio.csv', index=False)\r\n",
        "'''\r\n",
        "df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "df = df[(df['Stage'] == 'Closed Won') | (df['Stage'] == 'Closed Lost')]\r\n",
        "df = df[df['Opportunity_ID'] != 9773]\r\n",
        "df = df[['Opportunity_ID']]\r\n",
        "df.drop_duplicates(subset='Opportunity_ID', inplace=True)\r\n",
        "np.random.seed(1)\r\n",
        "drop_indices = np.random.choice(df.index, test_lines, replace=False)\r\n",
        "df.drop(drop_indices, inplace=True)\r\n",
        "predictions = model.predict(ds)\r\n",
        "df['Target'] = predictions\r\n",
        "df.to_csv('prediccionesFrioFrio.csv', index=False)\r\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndf = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\\ndf = df[(df['Stage'] == 'Closed Won') | (df['Stage'] == 'Closed Lost')]\\ndf = df[df['Opportunity_ID'] != 9773]\\ndf = df[['Opportunity_ID']]\\ndf.drop_duplicates(subset='Opportunity_ID', inplace=True)\\nnp.random.seed(1)\\ndrop_indices = np.random.choice(df.index, test_lines, replace=False)\\ndf.drop(drop_indices, inplace=True)\\npredictions = model.predict(ds)\\ndf['Target'] = predictions\\ndf.to_csv('prediccionesFrioFrio.csv', index=False)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}