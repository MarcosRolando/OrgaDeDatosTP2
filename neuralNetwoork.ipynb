{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralNetwoork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosRolando/OrgaDeDatosTP2/blob/main/neuralNetwoork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGS8tOq2uIJn"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import math as mt\r\n",
        "from tensorflow import feature_column\r\n"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JchCTb7pvWMj",
        "outputId": "a6f177e3-f607-4b4c-bc57-c310e5567d28"
      },
      "source": [
        "df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "df = df[df['Stage'].isin(['Closed Won', 'Closed Lost'])]\r\n",
        "df['Stage'] = df['Stage'].replace({'Closed Won':1, 'Closed Lost':0})\r\n",
        "df = df[df['Planned_Delivery_End_Date'] == 'NaT']\r\n",
        "df['Stage'].mean()"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6133333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y84PINJBAaX7"
      },
      "source": [
        "Tuneamos algunas cosas del DataFrame que notamos en el analisis de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxVjrxBo5slK"
      },
      "source": [
        "def preprocess_dataframe(df):\r\n",
        "\r\n",
        "  df.fillna(value=0, inplace=True) #Reemplazamos NAN por 0, ya que NAN rompe a Tensorflow\r\n",
        "\r\n",
        "  #Renombramos las columnas que tienen caracteres que TensorFlow no acepta como validos.\r\n",
        "  #Estos particularmente son whitespace, coma y parentesis por ejemplo.\r\n",
        "  df.rename(columns={'ASP_(converted)':'ASP_converted','Pricing, Delivery_Terms_Quote_Appr':\r\n",
        "                    'Pricing_Delivery_Terms_Quote_Appr','Pricing, Delivery_Terms_Approved':\r\n",
        "                    'Pricing_Delivery_Terms_Approved','Source ':'Source'},inplace=True)\r\n",
        "\r\n",
        "  df = df[df['Stage'].isin(['Closed Won', 'Closed Lost'])]\r\n",
        "  df.loc[:, 'Stage'].replace({'Closed Won':1, 'Closed Lost':0}, inplace=True) #0 corresponde a que el caso fue Closed Lost, 1 a que fue Closed Won. Asi tenemos un problema de clasificacion binario que puede entender la red neuronal.\r\n",
        "\r\n",
        "  df.loc[:, 'Planned_Delivery_Start_Date'] = pd.to_datetime(df['Planned_Delivery_Start_Date'], 'coerce',\r\n",
        "                                                                  format='%m/%d/%Y')\r\n",
        "  df.loc[:, 'Planned_Delivery_End_Date'] = pd.to_datetime(df['Planned_Delivery_End_Date'], 'coerce',\r\n",
        "                                                                                      format='%m/%d/%Y')\r\n",
        "  df = df[df['Opportunity_ID'] != 9773] #Hardcodeo este filtrado porque el id 9773 tiene mal cargada la fecha de delivery end, dando una diferencia de 200 anios xd\"\r\n",
        "\r\n",
        "  #Pongo .loc porque pandas me jode con warnings que son falsos positivos de slice copy\"\r\n",
        "  #Gracias Pandas!\"\r\n",
        "\r\n",
        "  #Creamos una nueva columna (Feature Engineering) que contiene la longitud en dias \r\n",
        "  #estimada de la operacion. En el informe habiamos encontrado que aparentaba haber\r\n",
        "  #una relacion cuadratica de decrecimiento a medida que aumentaban los dias donde disminuia\r\n",
        "  #la chance de completar la operacion.\r\n",
        "  df['Delta_Time'] = df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']\r\n",
        "  df.loc[:, 'Delta_Time'] = df['Delta_Time'].dt.days\r\n",
        "  df['Delta_Time'] = df['Delta_Time'].replace({np.nan:10.0})\r\n",
        "  df['Delta_Time'] = df.groupby('Opportunity_ID')['Delta_Time'].transform('max')\r\n",
        "\r\n",
        "  #Pasamos todo a dolares\r\n",
        "  currency_conversion = {'AUD':0.707612, 'EUR':1.131064, 'GBP':1.318055, 'JPY':0.008987, 'USD':1.0}\r\n",
        "  df['Total_Taxable_Amount_Currency'] = df[['Total_Taxable_Amount_Currency']].replace(currency_conversion)\r\n",
        "  df['Total_Taxable_Amount'] = df['Total_Taxable_Amount_Currency'] * df['Total_Taxable_Amount']\r\n",
        "\r\n",
        "  #Modifico la columna Brand para que en vez de decir que marca es, solo diga\r\n",
        "  #si tiene o no marca. Es importante aclarar que verificamos que siempre que una oportunidad\r\n",
        "  #tiene un producto con marca entonces todos sus productos tienen marca. Esto se cumple\r\n",
        "  #tanto en el set de entrenamiento como en el de test, por lo tanto al hacer drop_duplicates\r\n",
        "  #no nos va a pasar nunca el caso donde nos pudieramos quedar con una entrada de producto\r\n",
        "  #sin marca mientras que algun otro producto si tuviera, ya que confirmamos que o todos tienen\r\n",
        "  #marca o ninguno tiene.\r\n",
        "  df.loc[df['Brand'] == 'None', 'Brand'] = 'No'\r\n",
        "  df.loc[df['Brand'] != 'No', 'Brand'] = 'Yes'\r\n",
        "\r\n",
        "  #Agrego una columna que indica si tiene o no numero de contrato\r\n",
        "  df.loc[:, 'Sales_Contract_No'][df['Sales_Contract_No'] != 'None'] = 'Yes'\r\n",
        "  df.loc[:, 'Sales_Contract_No'][df['Sales_Contract_No'] == 'None'] = 'No'\r\n",
        "  df.rename(columns={'Sales_Contract_No':'Has_Contract_Number'}, inplace=True)\r\n",
        "\r\n",
        "  #Agrego una columna que indique la cantidad de productos que tiene esa\r\n",
        "  #oportunidad\r\n",
        "  df['Product_Name'] = 1\r\n",
        "  df['Product_Amount'] = df.groupby('Opportunity_ID')['Product_Name'].transform(lambda x: x.sum())\r\n",
        "\r\n",
        "  #Agrego una columna que indica si el owner de la cuenta es el mismo que el de la oportunidad\r\n",
        "  #o no\r\n",
        "  df['Same_Owner'] = (df['Account_Owner'] == df['Opportunity_Owner'])\r\n",
        "  df['Same_Owner'] = df['Same_Owner'].replace({False:'No', True:'Yes'})\r\n",
        "\r\n",
        "  #Agrego una columna que indica si tiene o no fecha de expiracion\r\n",
        "  df['Quote_Expiry_Date'] = (df['Quote_Expiry_Date'] != 'NaT')\r\n",
        "  df.rename(columns={'Quote_Expiry_Date':'Has_Expiry_Date'}, inplace=True)\r\n",
        "  df['Has_Expiry_Date'] = df['Has_Expiry_Date'].replace({True:'Yes',False:'No'})\r\n",
        "\r\n",
        "  #Reemplazo las 4 columnas de aprobacion por solo 2 columnas que indiquen si tuvo la aprobacion\r\n",
        "  #de delivery y burocratica o no. Recalco que si nunca la necesito seria equivalente a si\r\n",
        "  #la necesito y la consiguio.\r\n",
        "  df['Delivery_Approved'] = df['Pricing_Delivery_Terms_Quote_Appr'] + df['Pricing_Delivery_Terms_Approved']\r\n",
        "  df['Delivery_Approved'] = df['Delivery_Approved'].replace({0:1, 1:0, 2:1})\r\n",
        "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_0_Approval'] + df['Bureaucratic_Code_0_Approved']\r\n",
        "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_Approved'].replace({0:1, 1:0, 2:1})\r\n",
        "  df['Approved'] = df['Delivery_Approved'] | df['Bureaucratic_Code_Approved']\r\n",
        "\r\n",
        "  #Cambio TRF por una columna que es el valor medio de los TRF de la oportunidad\r\n",
        "  df[\"TRF\"] = df.groupby(\"Opportunity_ID\")[\"TRF\"].transform(\"mean\")\r\n",
        "\r\n",
        "  #Pruebo volar duplicados, solo cambia el producto. Si el producto no importa\r\n",
        "  #entonces volar duplicados no deberia importar. Obviamente vuelo el producto en el que\r\n",
        "  #quede tambien.\r\n",
        "  df.drop_duplicates('Opportunity_Name',inplace=True)\r\n",
        "  df.drop(columns=['Product_Name','Product_Family','Opportunity_Name'],inplace=True)\r\n",
        "\r\n",
        "  #Normalizo las columnas numericas\r\n",
        "  normalized_columns = ['ASP_converted','TRF','Total_Taxable_Amount', 'Product_Amount',\r\n",
        "                        'Delta_Time']\r\n",
        "  for column in normalized_columns:\r\n",
        "    df[column] = (df[column] - df[column].mean()) / df[column].std()\r\n",
        "\r\n",
        "  #Borro columnas que tengan el mismo dato en todas las entradas, o inconsecuentes como el ID / Opportunity_ID\r\n",
        "  #Algunas columnas borradas son porque pienso que no tienen incidencia, ir viendo.\r\n",
        "  #TODO: Analizar si el Sales_Contract_No no es que importe el numero en si, sino si tiene\r\n",
        "  #o no tiene numero de contrato. Por ahora no lo meto como input.\r\n",
        "  #TODO: Ver el mismo tema con la columna 'Price', la mayoria tiene None u Other\r\n",
        "  #y solo unos pocos tienen precio numerico. Quiza importe que tenga precio o no tenga,\r\n",
        "  #o si no tiene precio quiza importe si es None u Other. Por ahora no lo pongo\r\n",
        "  #como input.\r\n",
        "  df.drop(columns=['Submitted_for_Approval', 'Last_Activity', 'ASP_(converted)_Currency', \r\n",
        "                  'Prod_Category_A', 'ID', 'Opportunity_ID', 'Actual_Delivery_Date'],inplace=True)\r\n",
        "\r\n",
        "  #Drop columnas que quiza podamos usar pero por ahora no las uso\r\n",
        "  df.drop(columns=['Account_Created_Date','Opportunity_Created_Date',\r\n",
        "                  'Last_Modified_Date',\r\n",
        "                  'Planned_Delivery_Start_Date','Planned_Delivery_End_Date',\r\n",
        "                  'Month',\r\n",
        "                   'Delivery_Year','Region',\r\n",
        "                  'Price','ASP','ASP_Currency','Total_Amount_Currency',\r\n",
        "                  'Total_Amount','Total_Taxable_Amount_Currency','Currency',\r\n",
        "                   'Product_Category_B','Last_Modified_By', 'Account_Owner',\r\n",
        "                   'Opportunity_Owner','Account_Name','Product_Type','Size',\r\n",
        "                   'Territory', 'Billing_Country', 'Pricing_Delivery_Terms_Quote_Appr',\r\n",
        "                   'Pricing_Delivery_Terms_Approved', 'Bureaucratic_Code_0_Approval',\r\n",
        "                   'Bureaucratic_Code_0_Approved',\r\n",
        "                  'Delivery_Approved','Bureaucratic_Code_Approved',\r\n",
        "                   'Same_Owner','Total_Taxable_Amount','Product_Amount','ASP_converted',\r\n",
        "                   'Has_Expiry_Date','Delivery_Quarter','Quote_Type','Approved','TRF']\r\n",
        "                   ,inplace=True)\r\n",
        "\r\n",
        "  #Definimos que tipo de feature es cada columna\r\n",
        "\r\n",
        "  #Debemos separar algunos de los registros para armar un set de test propio (no el de la catedra). De esta forma sabremos rapidamente\r\n",
        "  #si nuestro modelo esta dando resultados optimos o no sin necesidad de estar subiendo el TP a Kaggle constantemente.\r\n",
        "  #Sin embargo, no queremos usar tantos registros ya que estariamos disminuyendo el set de entrenamiento considerablemente.\r\n",
        "  #Podemos empezar reservando 2000 registros para el test de prueba y ver que onda. Pasariamos de tener 16 mil a 14 mil \r\n",
        "  #registros para el set de entrenamiento, no es una perdida importantisima creo en principio, asi que arrancamos con eso.\r\n",
        "\r\n",
        "  #Por otro lado, nuestro test de prueba deberia tener un 50 50 de Closed Won y Closed Lost, por lo que no podemos elegir asi nomas\r\n",
        "  #al azar.\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjY1dvgAdSzw"
      },
      "source": [
        "# Metodo que pasa de DataFrame de Pandas a un DataSet de TensorFlow\r\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n",
        "  dataframe = dataframe.copy()\r\n",
        "  labels = dataframe.pop('Stage') #Retorna la columna Stage, eliminandolo simultaneamente del DataFrame. 'Stage' seria nuestra columna 'target', es decir, lo que queremos predecir.\r\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels)) #Crea un DataSet cuyos elementos son slices de los tensores pasados a la funcion. Ver documentacion oficial para comprender bien.\r\n",
        "  #En pocas palabras, le pasamos las columnas con los datos como diccionario (estilo 'columna':[dato1,dato2,dato3]) y una lista con los resultados estilo [resu1,resu2,resu3].\r\n",
        "  #Se genera un DataSet del estilo [('columna':[dato1,dato2,dato3], [resu1, resu2, resu3])].\r\n",
        "  #En realidad 'columna' es una lista de todas las columnas con sus correspondientes datos, pero se entiende la idea creo.\r\n",
        "  if shuffle:\r\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe)) #Al tener el buffer_size del mismo tamanio que la cantidad de datos del dataset, tenemos perfect shuffling (ver documentacion para comprender).\r\n",
        "    #Basicamente mezlcamos el dataset para que luego los batches que se armen contengan distintos elementos si lo entrenamos distintas veces.\r\n",
        "  ds = ds.batch(batch_size) #Arma batches de tamanio batch_size entre elementos consecutivos del DataSet.\r\n",
        "  return ds"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmaiJcl39e7X"
      },
      "source": [
        "#Arma las features, asignando las columnas del DataFrame segun corresponda al tipo de feature\r\n",
        "#(numerico, categorico, etc). Entiendase por feature a las columnas del DataFrame.\r\n",
        "def set_up_feature_columns(dataframe, numeric_columns, indicator_columns, bucket_columns, crossed_columns):\r\n",
        "  features = []\r\n",
        "\r\n",
        "  #numeric features\r\n",
        "  for column_name in numeric_columns:\r\n",
        "    features.append(feature_column.numeric_column(column_name))\r\n",
        "\r\n",
        "  #bucket features\r\n",
        "  boundaries = [] #En principio este boundary es solo para la columna 'Delta Time'. Ver de como generalizar.\r\n",
        "  for i in range(38):\r\n",
        "    boundaries.append(i*10.0)\r\n",
        "\r\n",
        "  for column_name in bucket_columns:\r\n",
        "    range_column = feature_column.numeric_column(column_name)\r\n",
        "    bukect_column = feature_column.bucketized_column(range_column, boundaries)\r\n",
        "    features.append(bukect_column)\r\n",
        "\r\n",
        "  #indicator features (one-hot value vector, para aquellas columnas categoricas de pocas opciones)\r\n",
        "  for column_name in indicator_columns:\r\n",
        "    categorical_column = feature_column.categorical_column_with_vocabulary_list(\r\n",
        "                                          column_name, dataframe[column_name].unique())\r\n",
        "    indicator_column = feature_column.indicator_column(categorical_column)\r\n",
        "    features.append(indicator_column)\r\n",
        "\r\n",
        "  #crossed features\r\n",
        "  for crossed_feature in crossed_columns:\r\n",
        "    categorical_columns = []\r\n",
        "    possible_values = 1\r\n",
        "    for column_name in crossed_feature:\r\n",
        "      column_keys = dataframe[column_name].unique()\r\n",
        "      possible_values = possible_values * len(column_keys)\r\n",
        "      categorical_columns.append(feature_column.categorical_column_with_vocabulary_list(\r\n",
        "                                            column_name, column_keys))\r\n",
        "    crossed_feature = feature_column.crossed_column(categorical_columns, hash_bucket_size=possible_values) #Ponemos la cantidad de buckets justos para representar todas las combinaciones\r\n",
        "    features.append(feature_column.indicator_column(crossed_feature))\r\n",
        "\r\n",
        "  return features"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0gHFhmD-AbE"
      },
      "source": [
        "Preparamos los features para el modelo, es decir, seteamos cada una de las columnas que vayamos a utilizar del DataFrame. Luego generamos el DataSet en base al DataFrame para darselo como input al modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bTkNirdLlCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c75777-2644-4ea5-bcf3-a640711fa201"
      },
      "source": [
        "  #Columnas que consideramos numericas\r\n",
        "  numeric_columns = []#['Total_Taxable_Amount','Product_Amount','ASP_converted','TRF']\r\n",
        "\r\n",
        "  #Columnas que consideramos clasificatorias con rango numerico\r\n",
        "  bucket_columns = ['Delta_Time']\r\n",
        "\r\n",
        "  #Columnas que consideramos categoricas de pocos valores posibles\r\n",
        "  indicator_columns = [#'Region', \r\n",
        "                       'Bureaucratic_Code','Source',#, 'Same_Owner',\r\n",
        "                        'Account_Type',\r\n",
        "                        'Opportunity_Type','Delivery_Terms',\r\n",
        "                       'Brand', 'Has_Contract_Number']#, 'Approved',\r\n",
        "                       #'Delivery_Quarter','Quote_Type',\r\n",
        "                        #'Has_Expiry_Date']\r\n",
        "                        #'Last_Modified_By'\r\n",
        "                        #'Product_Family', 'Product_Name',\r\n",
        "                        #'Account_Owner', 'Opportunity_Owner', 'Account_Name'\r\n",
        "                        #'Territory', 'Billing_Country'\r\n",
        "\r\n",
        "  #Columnas crossed, aquellas que queremos un parametro por combinacion\r\n",
        "  crossed_columns = []\r\n",
        "\r\n",
        "  df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "  df = preprocess_dataframe(df)\r\n",
        "\r\n",
        "  features = set_up_feature_columns(df,numeric_columns,indicator_columns,bucket_columns, crossed_columns)\r\n",
        "\r\n",
        "  #Separamos el DataFrame en uno de pruebo y el de entrenamiento. TODO: Ver el de validacion\r\n",
        "  test_lines = 200\r\n",
        "\r\n",
        "  np.random.seed(1)\r\n",
        "  drop_indices = np.random.choice(df.index, test_lines, replace=False)\r\n",
        "  df_test = df.loc[drop_indices, :]\r\n",
        "  df.drop(drop_indices, inplace=True)\r\n",
        "\r\n",
        "  #df_validation = df.tail(200)\r\n",
        "  #df.drop(df.tail(200).index, inplace=True)\r\n",
        "\r\n",
        "  feature_layer = tf.keras.layers.DenseFeatures(features)\r\n",
        "  ds = df_to_dataset(df,batch_size=56, shuffle=True)\r\n",
        "  ds_test = df_to_dataset(df_test, batch_size=56, shuffle=False)\r\n",
        "  #ds_validation = df_to_dataset(df_validation, batch_size=56, shuffle=False)\r\n",
        "  t = df['Delta_Time'].isna()\r\n",
        "  t.replace({False:0,True:1}, inplace=True)\r\n",
        "  t.sum()"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4582: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgLC9fhu-Mqu"
      },
      "source": [
        "Creamos y compilamos el modelo. En esta seccion se tunean las propiedades del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgXH6i3kL9ZN"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "  feature_layer,\r\n",
        "  tf.keras.layers.Dense(3, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgHkvYWxP2h"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stA0Y5lI9-Jy"
      },
      "source": [
        "Entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6px_EtjoYRXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f428645f-3882-4efb-9333-76f72149edb9"
      },
      "source": [
        "#Ignoren el WARNING, esta en la documentacion tambien. Nadie le da bola en StackOverflow xd.\r\n",
        "model.fit(ds, validation_data=ds_test, epochs=120)\r\n",
        "#model.fit(ds, epochs=120)\r\n",
        "model.summary()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Bureaucratic_Code': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>, 'Has_Contract_Number': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Delta_Time': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Bureaucratic_Code': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>, 'Has_Contract_Number': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Delta_Time': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "162/172 [===========================>..] - ETA: 0s - loss: 0.6943 - accuracy: 0.5539WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Bureaucratic_Code': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>, 'Has_Contract_Number': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Delta_Time': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "172/172 [==============================] - 2s 6ms/step - loss: 0.6916 - accuracy: 0.5600 - val_loss: 0.5740 - val_accuracy: 0.7450\n",
            "Epoch 2/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.5382 - accuracy: 0.8197 - val_loss: 0.4227 - val_accuracy: 0.8950\n",
            "Epoch 3/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.4117 - accuracy: 0.9079 - val_loss: 0.3264 - val_accuracy: 0.9650\n",
            "Epoch 4/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.3323 - accuracy: 0.9543 - val_loss: 0.2671 - val_accuracy: 0.9800\n",
            "Epoch 5/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.2886 - accuracy: 0.9614 - val_loss: 0.2299 - val_accuracy: 0.9800\n",
            "Epoch 6/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.2642 - accuracy: 0.9602 - val_loss: 0.2038 - val_accuracy: 0.9800\n",
            "Epoch 7/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.2431 - accuracy: 0.9596 - val_loss: 0.1847 - val_accuracy: 0.9800\n",
            "Epoch 8/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.2225 - accuracy: 0.9622 - val_loss: 0.1706 - val_accuracy: 0.9800\n",
            "Epoch 9/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.2071 - accuracy: 0.9629 - val_loss: 0.1590 - val_accuracy: 0.9800\n",
            "Epoch 10/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.2021 - accuracy: 0.9613 - val_loss: 0.1502 - val_accuracy: 0.9800\n",
            "Epoch 11/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1887 - accuracy: 0.9631 - val_loss: 0.1415 - val_accuracy: 0.9800\n",
            "Epoch 12/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1892 - accuracy: 0.9590 - val_loss: 0.1343 - val_accuracy: 0.9800\n",
            "Epoch 13/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1839 - accuracy: 0.9597 - val_loss: 0.1285 - val_accuracy: 0.9800\n",
            "Epoch 14/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1723 - accuracy: 0.9625 - val_loss: 0.1249 - val_accuracy: 0.9800\n",
            "Epoch 15/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1758 - accuracy: 0.9589 - val_loss: 0.1204 - val_accuracy: 0.9800\n",
            "Epoch 16/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1702 - accuracy: 0.9606 - val_loss: 0.1162 - val_accuracy: 0.9800\n",
            "Epoch 17/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1617 - accuracy: 0.9634 - val_loss: 0.1141 - val_accuracy: 0.9800\n",
            "Epoch 18/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1615 - accuracy: 0.9611 - val_loss: 0.1120 - val_accuracy: 0.9800\n",
            "Epoch 19/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1566 - accuracy: 0.9635 - val_loss: 0.1098 - val_accuracy: 0.9800\n",
            "Epoch 20/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1666 - accuracy: 0.9585 - val_loss: 0.1063 - val_accuracy: 0.9800\n",
            "Epoch 21/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1565 - accuracy: 0.9617 - val_loss: 0.1069 - val_accuracy: 0.9800\n",
            "Epoch 22/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1550 - accuracy: 0.9620 - val_loss: 0.1035 - val_accuracy: 0.9800\n",
            "Epoch 23/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1547 - accuracy: 0.9614 - val_loss: 0.1035 - val_accuracy: 0.9800\n",
            "Epoch 24/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1508 - accuracy: 0.9623 - val_loss: 0.1015 - val_accuracy: 0.9800\n",
            "Epoch 25/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1485 - accuracy: 0.9633 - val_loss: 0.1018 - val_accuracy: 0.9800\n",
            "Epoch 26/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1494 - accuracy: 0.9627 - val_loss: 0.1002 - val_accuracy: 0.9800\n",
            "Epoch 27/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1620 - accuracy: 0.9569 - val_loss: 0.0997 - val_accuracy: 0.9800\n",
            "Epoch 28/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1544 - accuracy: 0.9609 - val_loss: 0.1002 - val_accuracy: 0.9800\n",
            "Epoch 29/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1502 - accuracy: 0.9623 - val_loss: 0.0998 - val_accuracy: 0.9800\n",
            "Epoch 30/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1553 - accuracy: 0.9605 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 31/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1634 - accuracy: 0.9575 - val_loss: 0.0987 - val_accuracy: 0.9800\n",
            "Epoch 32/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1503 - accuracy: 0.9617 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 33/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1533 - accuracy: 0.9601 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "Epoch 34/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1530 - accuracy: 0.9619 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 35/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1524 - accuracy: 0.9607 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 36/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1498 - accuracy: 0.9614 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 37/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1600 - accuracy: 0.9583 - val_loss: 0.0977 - val_accuracy: 0.9800\n",
            "Epoch 38/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1629 - accuracy: 0.9580 - val_loss: 0.0963 - val_accuracy: 0.9800\n",
            "Epoch 39/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1455 - accuracy: 0.9617 - val_loss: 0.0981 - val_accuracy: 0.9800\n",
            "Epoch 40/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1464 - accuracy: 0.9630 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 41/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1595 - accuracy: 0.9574 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 42/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1466 - accuracy: 0.9628 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 43/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1529 - accuracy: 0.9612 - val_loss: 0.0967 - val_accuracy: 0.9800\n",
            "Epoch 44/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1542 - accuracy: 0.9605 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 45/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1496 - accuracy: 0.9625 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 46/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1458 - accuracy: 0.9631 - val_loss: 0.0963 - val_accuracy: 0.9800\n",
            "Epoch 47/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1436 - accuracy: 0.9634 - val_loss: 0.0960 - val_accuracy: 0.9800\n",
            "Epoch 48/120\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.1497 - accuracy: 0.9623 - val_loss: 0.0961 - val_accuracy: 0.9800\n",
            "Epoch 49/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1449 - accuracy: 0.9632 - val_loss: 0.0956 - val_accuracy: 0.9800\n",
            "Epoch 50/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1507 - accuracy: 0.9612 - val_loss: 0.0953 - val_accuracy: 0.9800\n",
            "Epoch 51/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1466 - accuracy: 0.9616 - val_loss: 0.0987 - val_accuracy: 0.9800\n",
            "Epoch 52/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1488 - accuracy: 0.9622 - val_loss: 0.0958 - val_accuracy: 0.9800\n",
            "Epoch 53/120\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.1435 - accuracy: 0.9629 - val_loss: 0.0978 - val_accuracy: 0.9800\n",
            "Epoch 54/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1496 - accuracy: 0.9616 - val_loss: 0.0960 - val_accuracy: 0.9800\n",
            "Epoch 55/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1663 - accuracy: 0.9567 - val_loss: 0.0957 - val_accuracy: 0.9800\n",
            "Epoch 56/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1471 - accuracy: 0.9622 - val_loss: 0.0955 - val_accuracy: 0.9800\n",
            "Epoch 57/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.9644 - val_loss: 0.0953 - val_accuracy: 0.9800\n",
            "Epoch 58/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1553 - accuracy: 0.9591 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 59/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1414 - accuracy: 0.9638 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 60/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1569 - accuracy: 0.9595 - val_loss: 0.0960 - val_accuracy: 0.9800\n",
            "Epoch 61/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1498 - accuracy: 0.9617 - val_loss: 0.0978 - val_accuracy: 0.9800\n",
            "Epoch 62/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1479 - accuracy: 0.9613 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Epoch 63/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1570 - accuracy: 0.9582 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 64/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1469 - accuracy: 0.9615 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 65/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1499 - accuracy: 0.9612 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 66/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1505 - accuracy: 0.9601 - val_loss: 0.0968 - val_accuracy: 0.9800\n",
            "Epoch 67/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.9647 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 68/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1561 - accuracy: 0.9581 - val_loss: 0.0953 - val_accuracy: 0.9800\n",
            "Epoch 69/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1501 - accuracy: 0.9610 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 70/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1430 - accuracy: 0.9627 - val_loss: 0.0962 - val_accuracy: 0.9800\n",
            "Epoch 71/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1517 - accuracy: 0.9597 - val_loss: 0.0961 - val_accuracy: 0.9800\n",
            "Epoch 72/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1449 - accuracy: 0.9619 - val_loss: 0.0962 - val_accuracy: 0.9800\n",
            "Epoch 73/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.9645 - val_loss: 0.0951 - val_accuracy: 0.9800\n",
            "Epoch 74/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1531 - accuracy: 0.9589 - val_loss: 0.0954 - val_accuracy: 0.9800\n",
            "Epoch 75/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.9635 - val_loss: 0.0957 - val_accuracy: 0.9800\n",
            "Epoch 76/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.9614 - val_loss: 0.0949 - val_accuracy: 0.9800\n",
            "Epoch 77/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1473 - accuracy: 0.9610 - val_loss: 0.0967 - val_accuracy: 0.9800\n",
            "Epoch 78/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1536 - accuracy: 0.9586 - val_loss: 0.0953 - val_accuracy: 0.9800\n",
            "Epoch 79/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1469 - accuracy: 0.9610 - val_loss: 0.0938 - val_accuracy: 0.9800\n",
            "Epoch 80/120\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.1468 - accuracy: 0.9604 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 81/120\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.1389 - accuracy: 0.9647 - val_loss: 0.0959 - val_accuracy: 0.9800\n",
            "Epoch 82/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1381 - accuracy: 0.9640 - val_loss: 0.0939 - val_accuracy: 0.9800\n",
            "Epoch 83/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1515 - accuracy: 0.9587 - val_loss: 0.0954 - val_accuracy: 0.9800\n",
            "Epoch 84/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.9639 - val_loss: 0.0953 - val_accuracy: 0.9800\n",
            "Epoch 85/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1516 - accuracy: 0.9589 - val_loss: 0.0951 - val_accuracy: 0.9800\n",
            "Epoch 86/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1547 - accuracy: 0.9582 - val_loss: 0.0942 - val_accuracy: 0.9800\n",
            "Epoch 87/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1464 - accuracy: 0.9616 - val_loss: 0.0945 - val_accuracy: 0.9800\n",
            "Epoch 88/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1403 - accuracy: 0.9619 - val_loss: 0.0953 - val_accuracy: 0.9800\n",
            "Epoch 89/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1462 - accuracy: 0.9616 - val_loss: 0.0959 - val_accuracy: 0.9800\n",
            "Epoch 90/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1385 - accuracy: 0.9629 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Epoch 91/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.9627 - val_loss: 0.0956 - val_accuracy: 0.9800\n",
            "Epoch 92/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1490 - accuracy: 0.9614 - val_loss: 0.0955 - val_accuracy: 0.9800\n",
            "Epoch 93/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1498 - accuracy: 0.9590 - val_loss: 0.0931 - val_accuracy: 0.9800\n",
            "Epoch 94/120\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.1529 - accuracy: 0.9590 - val_loss: 0.0932 - val_accuracy: 0.9800\n",
            "Epoch 95/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1466 - accuracy: 0.9598 - val_loss: 0.0951 - val_accuracy: 0.9800\n",
            "Epoch 96/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1425 - accuracy: 0.9618 - val_loss: 0.0956 - val_accuracy: 0.9800\n",
            "Epoch 97/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1543 - accuracy: 0.9586 - val_loss: 0.0933 - val_accuracy: 0.9800\n",
            "Epoch 98/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1425 - accuracy: 0.9626 - val_loss: 0.0955 - val_accuracy: 0.9800\n",
            "Epoch 99/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1567 - accuracy: 0.9579 - val_loss: 0.0933 - val_accuracy: 0.9800\n",
            "Epoch 100/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1487 - accuracy: 0.9593 - val_loss: 0.0931 - val_accuracy: 0.9800\n",
            "Epoch 101/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1450 - accuracy: 0.9615 - val_loss: 0.0945 - val_accuracy: 0.9800\n",
            "Epoch 102/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1485 - accuracy: 0.9601 - val_loss: 0.0934 - val_accuracy: 0.9800\n",
            "Epoch 103/120\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.1396 - accuracy: 0.9631 - val_loss: 0.0929 - val_accuracy: 0.9800\n",
            "Epoch 104/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.9639 - val_loss: 0.0933 - val_accuracy: 0.9800\n",
            "Epoch 105/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.9610 - val_loss: 0.0927 - val_accuracy: 0.9800\n",
            "Epoch 106/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1559 - accuracy: 0.9579 - val_loss: 0.0931 - val_accuracy: 0.9800\n",
            "Epoch 107/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1476 - accuracy: 0.9589 - val_loss: 0.0924 - val_accuracy: 0.9800\n",
            "Epoch 108/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.9635 - val_loss: 0.0939 - val_accuracy: 0.9800\n",
            "Epoch 109/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1482 - accuracy: 0.9595 - val_loss: 0.0920 - val_accuracy: 0.9800\n",
            "Epoch 110/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1518 - accuracy: 0.9585 - val_loss: 0.0918 - val_accuracy: 0.9800\n",
            "Epoch 111/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1429 - accuracy: 0.9626 - val_loss: 0.0926 - val_accuracy: 0.9800\n",
            "Epoch 112/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.9627 - val_loss: 0.0955 - val_accuracy: 0.9800\n",
            "Epoch 113/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.9584 - val_loss: 0.0932 - val_accuracy: 0.9800\n",
            "Epoch 114/120\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.1474 - accuracy: 0.9609 - val_loss: 0.0915 - val_accuracy: 0.9800\n",
            "Epoch 115/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.9625 - val_loss: 0.0916 - val_accuracy: 0.9800\n",
            "Epoch 116/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1397 - accuracy: 0.9626 - val_loss: 0.0923 - val_accuracy: 0.9800\n",
            "Epoch 117/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1437 - accuracy: 0.9611 - val_loss: 0.0930 - val_accuracy: 0.9800\n",
            "Epoch 118/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1477 - accuracy: 0.9603 - val_loss: 0.0908 - val_accuracy: 0.9800\n",
            "Epoch 119/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1480 - accuracy: 0.9599 - val_loss: 0.0914 - val_accuracy: 0.9800\n",
            "Epoch 120/120\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 0.1440 - accuracy: 0.9616 - val_loss: 0.0917 - val_accuracy: 0.9800\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_features_28 (DenseFeat multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             multiple                  318       \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             multiple                  4         \n",
            "=================================================================\n",
            "Total params: 322\n",
            "Trainable params: 322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYTFqZSM-TA-"
      },
      "source": [
        "Evaluamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SDVpkRXMmLD"
      },
      "source": [
        "#model.evaluate(ds_test)"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1z7YhHrAGLc"
      },
      "source": [
        "Escribimos las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8BZJqotAFqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "c38b383e-81c0-4e61-82ad-8e1799bb036f"
      },
      "source": [
        "\r\n",
        "frio_test_df = pd.read_csv('/content/Test_TP2_Datos_2020-2C.csv')\r\n",
        "frio_test_df['Stage'] = 'Closed Won' #Esto esta solo para que funque todo, no lo uso. No se bien como armarlo sin los labels de Stage. TODO: Averiguar como es!\r\n",
        "aux_df = frio_test_df[['Opportunity_ID']] #Esta columna la vuela el preprocesado sino\r\n",
        "frio_test_df = preprocess_dataframe(frio_test_df)\r\n",
        "frio_test_ds = df_to_dataset(frio_test_df, shuffle=False, batch_size=56)\r\n",
        "predictions = model.predict(frio_test_ds)\r\n",
        "\r\n",
        "aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True) #Lo hacia el preprocesado pero es verdad que lo copie antes a este xd, perdon Agus, paja de dejarlo lindo.\r\n",
        "aux_df['Target'] = predictions\r\n",
        "\r\n",
        "#aux_df['Target'] = aux_df.groupby(by='Opportunity_ID').transform(lambda x: x.mean())\r\n",
        "#aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True)\r\n",
        "\r\n",
        "aux_df.to_csv('prediccionesFrioFrio.csv', index=False)\r\n",
        "'''\r\n",
        "df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "df = df[(df['Stage'] == 'Closed Won') | (df['Stage'] == 'Closed Lost')]\r\n",
        "df = df[df['Opportunity_ID'] != 9773]\r\n",
        "df = df[['Opportunity_ID']]\r\n",
        "df.drop_duplicates(subset='Opportunity_ID', inplace=True)\r\n",
        "np.random.seed(1)\r\n",
        "drop_indices = np.random.choice(df.index, test_lines, replace=False)\r\n",
        "df.drop(drop_indices, inplace=True)\r\n",
        "predictions = model.predict(ds)\r\n",
        "df['Target'] = predictions\r\n",
        "df.to_csv('prediccionesFrioFrio.csv', index=False)\r\n",
        "'''"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndf = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\\ndf = df[(df['Stage'] == 'Closed Won') | (df['Stage'] == 'Closed Lost')]\\ndf = df[df['Opportunity_ID'] != 9773]\\ndf = df[['Opportunity_ID']]\\ndf.drop_duplicates(subset='Opportunity_ID', inplace=True)\\nnp.random.seed(1)\\ndrop_indices = np.random.choice(df.index, test_lines, replace=False)\\ndf.drop(drop_indices, inplace=True)\\npredictions = model.predict(ds)\\ndf['Target'] = predictions\\ndf.to_csv('prediccionesFrioFrio.csv', index=False)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    }
  ]
}