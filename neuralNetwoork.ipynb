{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neuralNetwoork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosRolando/OrgaDeDatosTP2/blob/main/neuralNetwoork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGS8tOq2uIJn"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import math as mt\r\n",
        "import regex as re\r\n",
        "from tensorflow import feature_column\r\n"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWNXW4INdM5s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "db81d8ae-1906-45b0-94b4-a4ba9c51d4c5"
      },
      "source": [
        "'''\r\n",
        "df1 = pd.read_csv('/content/prediccionesFrioFrio (1).csv')\r\n",
        "df2 = pd.read_csv('/content/prediccionesFrioFrio (2).csv')\r\n",
        "\r\n",
        "dfMerge = df1\r\n",
        "target = []\r\n",
        "\r\n",
        "for i in df1['Opportunity_ID']:\r\n",
        "  if df1.loc[df1['Opportunity_ID'] == i, 'Target'].item() >= 0.5:\r\n",
        "    target.append(max(df1.loc[df1['Opportunity_ID'] == i, 'Target'].item(), df2.loc[df2['Opportunity_ID'] == i, 'Target'].item()))\r\n",
        "  else:\r\n",
        "    target.append(max(df1.loc[df1['Opportunity_ID'] == i, 'Target'].item(), df2.loc[df2['Opportunity_ID'] == i, 'Target'].item()))\r\n",
        "\r\n",
        "dfMerge['Target'] = target\r\n",
        "\r\n",
        "dfMerge.to_csv('prediccionesFrioFrioMerge.csv', index=False)\r\n",
        "'''"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndf1 = pd.read_csv('/content/prediccionesFrioFrio (1).csv')\\ndf2 = pd.read_csv('/content/prediccionesFrioFrio (2).csv')\\n\\ndfMerge = df1\\ntarget = []\\n\\nfor i in df1['Opportunity_ID']:\\n  if df1.loc[df1['Opportunity_ID'] == i, 'Target'].item() >= 0.5:\\n    target.append(max(df1.loc[df1['Opportunity_ID'] == i, 'Target'].item(), df2.loc[df2['Opportunity_ID'] == i, 'Target'].item()))\\n  else:\\n    target.append(max(df1.loc[df1['Opportunity_ID'] == i, 'Target'].item(), df2.loc[df2['Opportunity_ID'] == i, 'Target'].item()))\\n\\ndfMerge['Target'] = target\\n\\ndfMerge.to_csv('prediccionesFrioFrioMerge.csv', index=False)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JchCTb7pvWMj",
        "outputId": "36f79752-1937-42f6-fb46-78ebc14a33f4"
      },
      "source": [
        "df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "\r\n",
        "#df = df[df['Brand'] == 'No']\r\n",
        "'''\r\n",
        "df['days'] = (df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']).dt.days\r\n",
        "df['days'] = df.groupby('Opportunity_ID')['days'].transform('max')\r\n",
        "df['days'] = df['days'].replace({np.nan:10.0}) #Reemplazo con 10 porque los que no tienen fecha final ganan el 60%, y el analisis de los datos da que el 60% es maso a los 10 dias. Asi no jodo el resto de los datos\r\n",
        "df['days'] = df['days'] > 0\r\n",
        "\r\n",
        "df['TRF'] = df.groupby('Opportunity_ID')['TRF'].transform('max')\r\n",
        "\r\n",
        "df = df[(df['Has_Contract_Number'] == 'Yes')]\r\n",
        "df['Ratio'] = df.groupby('Territory')['Stage'].transform('mean')\r\n",
        "df[df['Ratio'] < 0.8]['Territory'].value_counts()'''\r\n",
        "#df[df['Products'] == 'Product_Name_111']\r\n",
        "df['Product_Family'].value_counts()\r\n"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Product_Family_77     1345\n",
              "Product_Family_133    1249\n",
              "Product_Family_132     763\n",
              "Product_Family_212     636\n",
              "Product_Family_100     619\n",
              "                      ... \n",
              "Product_Family_182       1\n",
              "Product_Family_242       1\n",
              "Product_Family_241       1\n",
              "Product_Family_124       1\n",
              "Product_Family_103       1\n",
              "Name: Product_Family, Length: 227, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y84PINJBAaX7"
      },
      "source": [
        "Tuneamos algunas cosas del DataFrame que notamos en el analisis de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxVjrxBo5slK"
      },
      "source": [
        "def preprocess_dataframe(df):\r\n",
        "\r\n",
        "  df.fillna(value=0, inplace=True) #Reemplazamos NAN por 0, ya que NAN rompe a Tensorflow\r\n",
        "\r\n",
        "  #Renombramos las columnas que tienen caracteres que TensorFlow no acepta como validos.\r\n",
        "  #Estos particularmente son whitespace, coma y parentesis por ejemplo.\r\n",
        "  df.rename(columns={'ASP_(converted)':'ASP_converted','Pricing, Delivery_Terms_Quote_Appr':\r\n",
        "                    'Pricing_Delivery_Terms_Quote_Appr','Pricing, Delivery_Terms_Approved':\r\n",
        "                    'Pricing_Delivery_Terms_Approved','Source ':'Source'},inplace=True)\r\n",
        "\r\n",
        "  df = df[df['Stage'].isin(['Closed Won', 'Closed Lost'])]\r\n",
        "  df.loc[:, 'Stage'].replace({'Closed Won':1, 'Closed Lost':0}, inplace=True) #0 corresponde a que el caso fue Closed Lost, 1 a que fue Closed Won. Asi tenemos un problema de clasificacion binario que puede entender la red neuronal.\r\n",
        "\r\n",
        "  df.loc[:, 'Planned_Delivery_Start_Date'] = pd.to_datetime(df['Planned_Delivery_Start_Date'], 'coerce',\r\n",
        "                                                                  format='%m/%d/%Y')\r\n",
        "  df.loc[:, 'Planned_Delivery_End_Date'] = pd.to_datetime(df['Planned_Delivery_End_Date'], 'coerce',\r\n",
        "                                                                                      format='%m/%d/%Y')\r\n",
        "  df = df[df['Opportunity_ID'] != 9773] #Hardcodeo este filtrado porque el id 9773 tiene mal cargada la fecha de delivery end, dando una diferencia de 200 anios xd\"\r\n",
        "\r\n",
        "  #Pongo .loc porque pandas me jode con warnings que son falsos positivos de slice copy\"\r\n",
        "  #Gracias Pandas!\"\r\n",
        "\r\n",
        "  #Creamos una nueva columna (Feature Engineering) que contiene la longitud en dias \r\n",
        "  #estimada de la operacion. En el informe habiamos encontrado que aparentaba haber\r\n",
        "  #una relacion cuadratica de decrecimiento a medida que aumentaban los dias donde disminuia\r\n",
        "  #la chance de completar la operacion.\r\n",
        "  df['Delta_Time'] = df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']\r\n",
        "  df.loc[:, 'Delta_Time'] = df['Delta_Time'].dt.days\r\n",
        "  df['Delta_Time'] = df['Delta_Time'].replace({np.nan:10.0}) #Reemplazo con 10 porque los que no tienen fecha final ganan el 60%, y el analisis de los datos da que el 60% es maso a los 10 dias. Asi no jodo el resto de los datos\r\n",
        "  df['Delta_Time'] = df.groupby('Opportunity_ID')['Delta_Time'].transform('max')\r\n",
        "\r\n",
        "  #Pasamos todo a dolares\r\n",
        "  currency_conversion = {'AUD':0.707612, 'EUR':1.131064, 'GBP':1.318055, 'JPY':0.008987, 'USD':1.0}\r\n",
        "  df['Total_Taxable_Amount_Currency'] = df[['Total_Taxable_Amount_Currency']].replace(currency_conversion)\r\n",
        "  df['Total_Taxable_Amount'] = df['Total_Taxable_Amount_Currency'] * df['Total_Taxable_Amount']\r\n",
        "\r\n",
        "  #Modifico la columna Brand para que en vez de decir que marca es, solo diga\r\n",
        "  #si tiene o no marca. Es importante aclarar que verificamos que siempre que una oportunidad\r\n",
        "  #tiene un producto con marca entonces todos sus productos tienen marca. Esto se cumple\r\n",
        "  #tanto en el set de entrenamiento como en el de test, por lo tanto al hacer drop_duplicates\r\n",
        "  #no nos va a pasar nunca el caso donde nos pudieramos quedar con una entrada de producto\r\n",
        "  #sin marca mientras que algun otro producto si tuviera, ya que confirmamos que o todos tienen\r\n",
        "  #marca o ninguno tiene.\r\n",
        "  df.loc[df['Brand'] == 'None', 'Brand'] = 'No'\r\n",
        "  df.loc[df['Brand'] != 'No', 'Brand'] = 'Yes'\r\n",
        "\r\n",
        "  #Agrego una columna que indica si tiene o no numero de contrato\r\n",
        "  df.loc[:, 'Sales_Contract_No'][df['Sales_Contract_No'] != 'None'] = 'Yes'\r\n",
        "  df.loc[:, 'Sales_Contract_No'][df['Sales_Contract_No'] == 'None'] = 'No'\r\n",
        "  df.rename(columns={'Sales_Contract_No':'Has_Contract_Number'}, inplace=True)\r\n",
        "\r\n",
        "  #Agrego una columna que indique la cantidad de productos que tiene esa\r\n",
        "  #oportunidad\r\n",
        "  df['Product'] = 1\r\n",
        "  df['Product_Amount'] = df.groupby('Opportunity_ID')['Product'].transform(lambda x: x.sum())\r\n",
        "\r\n",
        "  #Agrego una columna que indica si el owner de la cuenta es el mismo que el de la oportunidad\r\n",
        "  #o no\r\n",
        "  df['Same_Owner'] = (df['Account_Owner'] == df['Opportunity_Owner'])\r\n",
        "  df['Same_Owner'] = df['Same_Owner'].replace({False:'No', True:'Yes'})\r\n",
        "\r\n",
        "  #Agrego columna que indica si el ultimo que modifico la oportunidad es el mismo que el opportunity owner\r\n",
        "  df['Same_Owner_Modifier'] = (df['Last_Modified_By'] == df['Opportunity_Owner'])\r\n",
        "  df['Same_Owner_Modifier'] = df['Same_Owner_Modifier'].replace({False:'No', True:'Yes'})\r\n",
        "\r\n",
        "  #Agrego una columna que indica si tiene o no fecha de expiracion\r\n",
        "  df['Quote_Expiry_Date'] = (df['Quote_Expiry_Date'] != 'NaT')\r\n",
        "  df.rename(columns={'Quote_Expiry_Date':'Has_Expiry_Date'}, inplace=True)\r\n",
        "  df['Has_Expiry_Date'] = df['Has_Expiry_Date'].replace({True:'Yes',False:'No'})\r\n",
        "\r\n",
        "  #Reemplazo las 4 columnas de aprobacion por solo 2 columnas que indiquen si tuvo la aprobacion\r\n",
        "  #de delivery y burocratica o no. Recalco que si nunca la necesito seria equivalente a si\r\n",
        "  #la necesito y la consiguio.\r\n",
        "  df['Delivery_Approved'] = df['Pricing_Delivery_Terms_Quote_Appr'] + df['Pricing_Delivery_Terms_Approved']\r\n",
        "  df['Delivery_Approved'] = df['Delivery_Approved'].replace({0:1, 1:0, 2:1})\r\n",
        "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_0_Approval'] + df['Bureaucratic_Code_0_Approved']\r\n",
        "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_Approved'].replace({0:1, 1:0, 2:1})\r\n",
        "  df['Approved'] = df['Delivery_Approved'] & df['Bureaucratic_Code_Approved']\r\n",
        "\r\n",
        "  #Cambio TRF por una columna que es el valor medio de los TRF de la oportunidad\r\n",
        "  df[\"TRF\"] = df.groupby(\"Opportunity_ID\")[\"TRF\"].transform(\"mean\")\r\n",
        "\r\n",
        "  def combineProducts(x):\r\n",
        "    products = \"\"\r\n",
        "    added = []\r\n",
        "    for product in x:\r\n",
        "      product = re.findall('\\d+', product)[0]\r\n",
        "      if added.count(product) == 0:\r\n",
        "        products += (product)\r\n",
        "        added.append(product)\r\n",
        "    return products\r\n",
        "\r\n",
        "  #Junto todos los productos en una sola entrada\r\n",
        "  df['Products'] = df.groupby('Opportunity_ID')['Product_Family'].transform(combineProducts)\r\n",
        "\r\n",
        "  #Pruebo volar duplicados, solo cambia el producto. Si el producto no importa\r\n",
        "  #entonces volar duplicados no deberia importar. Obviamente vuelo el producto en el que\r\n",
        "  #quede tambien.\r\n",
        "  df.drop_duplicates('Opportunity_Name',inplace=True)\r\n",
        "  df.drop(columns=['Product_Name','Product_Family','Opportunity_Name'],inplace=True)\r\n",
        "\r\n",
        "  #Normalizo las columnas numericas\r\n",
        "  normalized_columns = ['ASP_converted','TRF','Total_Taxable_Amount', 'Product_Amount']\r\n",
        "  for column in normalized_columns:\r\n",
        "    df[column] = (df[column] - df[column].mean()) / df[column].std()\r\n",
        "\r\n",
        "  #Borro columnas que tengan el mismo dato en todas las entradas, o inconsecuentes como el ID / Opportunity_ID\r\n",
        "  #Algunas columnas borradas son porque pienso que no tienen incidencia, ir viendo.\r\n",
        "  #TODO: Analizar si el Sales_Contract_No no es que importe el numero en si, sino si tiene\r\n",
        "  #o no tiene numero de contrato. Por ahora no lo meto como input.\r\n",
        "  #TODO: Ver el mismo tema con la columna 'Price', la mayoria tiene None u Other\r\n",
        "  #y solo unos pocos tienen precio numerico. Quiza importe que tenga precio o no tenga,\r\n",
        "  #o si no tiene precio quiza importe si es None u Other. Por ahora no lo pongo\r\n",
        "  #como input.\r\n",
        "  df.drop(columns=['Submitted_for_Approval', 'Last_Activity', 'ASP_(converted)_Currency', \r\n",
        "                  'Prod_Category_A', 'ID', 'Opportunity_ID', \r\n",
        "                   'Actual_Delivery_Date'],inplace=True)\r\n",
        "\r\n",
        "  #Drop columnas que quiza podamos usar pero por ahora no las uso\r\n",
        "  df.drop(columns=['Account_Created_Date','Opportunity_Created_Date',\r\n",
        "                  'Last_Modified_Date',\r\n",
        "                  'Planned_Delivery_Start_Date','Planned_Delivery_End_Date',\r\n",
        "                  'Month',\r\n",
        "                  'Delivery_Year',\r\n",
        "                  'Price','ASP','ASP_Currency','Total_Amount_Currency',\r\n",
        "                  'Total_Amount','Total_Taxable_Amount_Currency','Currency',\r\n",
        "                   'Product_Category_B','Last_Modified_By', 'Account_Owner',\r\n",
        "                   'Opportunity_Owner','Account_Name','Product_Type','Size',\r\n",
        "                   'Billing_Country',\r\n",
        "                   'Approved', 'Has_Contract_Number','Territory',\r\n",
        "                   'Product', 'Products']\r\n",
        "                   ,inplace=True)\r\n",
        "  \r\n",
        "  #Definimos que tipo de feature es cada columna\r\n",
        "\r\n",
        "  #Debemos separar algunos de los registros para armar un set de test propio (no el de la catedra). De esta forma sabremos rapidamente\r\n",
        "  #si nuestro modelo esta dando resultados optimos o no sin necesidad de estar subiendo el TP a Kaggle constantemente.\r\n",
        "  #Sin embargo, no queremos usar tantos registros ya que estariamos disminuyendo el set de entrenamiento considerablemente.\r\n",
        "  #Podemos empezar reservando 2000 registros para el test de prueba y ver que onda. Pasariamos de tener 16 mil a 14 mil \r\n",
        "  #registros para el set de entrenamiento, no es una perdida importantisima creo en principio, asi que arrancamos con eso.\r\n",
        "\r\n",
        "  #Por otro lado, nuestro test de prueba deberia tener un 50 50 de Closed Won y Closed Lost, por lo que no podemos elegir asi nomas\r\n",
        "  #al azar.\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjY1dvgAdSzw"
      },
      "source": [
        "# Metodo que pasa de DataFrame de Pandas a un DataSet de TensorFlow\r\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n",
        "  dataframe = dataframe.copy()\r\n",
        "  labels = dataframe.pop('Stage') #Retorna la columna Stage, eliminandolo simultaneamente del DataFrame. 'Stage' seria nuestra columna 'target', es decir, lo que queremos predecir.\r\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels)) #Crea un DataSet cuyos elementos son slices de los tensores pasados a la funcion. Ver documentacion oficial para comprender bien.\r\n",
        "  #En pocas palabras, le pasamos las columnas con los datos como diccionario (estilo 'columna':[dato1,dato2,dato3]) y una lista con los resultados estilo [resu1,resu2,resu3].\r\n",
        "  #Se genera un DataSet del estilo [('columna':[dato1,dato2,dato3], [resu1, resu2, resu3])].\r\n",
        "  #En realidad 'columna' es una lista de todas las columnas con sus correspondientes datos, pero se entiende la idea creo.\r\n",
        "  if shuffle:\r\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe)) #Al tener el buffer_size del mismo tamanio que la cantidad de datos del dataset, tenemos perfect shuffling (ver documentacion para comprender).\r\n",
        "    #Basicamente mezlcamos el dataset para que luego los batches que se armen contengan distintos elementos si lo entrenamos distintas veces.\r\n",
        "  ds = ds.batch(batch_size) #Arma batches de tamanio batch_size entre elementos consecutivos del DataSet.\r\n",
        "  return ds"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmaiJcl39e7X"
      },
      "source": [
        "#Arma las features, asignando las columnas del DataFrame segun corresponda al tipo de feature\r\n",
        "#(numerico, categorico, etc). Entiendase por feature a las columnas del DataFrame.\r\n",
        "def set_up_feature_columns(dataframe, numeric_columns, indicator_columns, bucket_columns, crossed_columns, hashed_columns):\r\n",
        "  features = []\r\n",
        "\r\n",
        "  #numeric features\r\n",
        "  for column_name in numeric_columns:\r\n",
        "    features.append(feature_column.numeric_column(column_name))\r\n",
        "\r\n",
        "  #bucket features\r\n",
        "  boundaries = [] #En principio este boundary es solo para la columna 'Delta Time'. Ver de como generalizar.\r\n",
        "  for i in range(37):\r\n",
        "    boundaries.append(i*10.0)\r\n",
        "\r\n",
        "  for column_name in bucket_columns:\r\n",
        "    range_column = feature_column.numeric_column(column_name)\r\n",
        "    bukect_column = feature_column.bucketized_column(range_column, boundaries)\r\n",
        "    features.append(bukect_column)\r\n",
        "\r\n",
        "  #indicator features (one-hot value vector, para aquellas columnas categoricas de pocas opciones)\r\n",
        "  for column_name in indicator_columns:\r\n",
        "    categorical_column = feature_column.categorical_column_with_vocabulary_list(\r\n",
        "                                          column_name, dataframe[column_name].unique())\r\n",
        "    indicator_column = feature_column.indicator_column(categorical_column)\r\n",
        "    features.append(indicator_column)\r\n",
        "\r\n",
        "  #crossed features\r\n",
        "  for crossed_feature in crossed_columns:\r\n",
        "    categorical_columns = []\r\n",
        "    possible_values = 1\r\n",
        "    for column_name in crossed_feature:\r\n",
        "      column_keys = dataframe[column_name].unique()\r\n",
        "      possible_values = possible_values * len(column_keys)\r\n",
        "      categorical_columns.append(feature_column.categorical_column_with_vocabulary_list(\r\n",
        "                                            column_name, column_keys))\r\n",
        "    crossed_feature = feature_column.crossed_column(categorical_columns, hash_bucket_size=possible_values*10) #Ponemos mas valores de los necesarios para tratar de garantizar que no haya colisiones\r\n",
        "    features.append(feature_column.indicator_column(crossed_feature))\r\n",
        "\r\n",
        "\r\n",
        "  #hashed features\r\n",
        "  for column_name in hashed_columns:\r\n",
        "    hashed_column = feature_column.categorical_column_with_hash_bucket(\r\n",
        "                  column_name, hash_bucket_size=100)\r\n",
        "    features.append(feature_column.indicator_column(hashed_column))\r\n",
        "\r\n",
        "  return features"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0gHFhmD-AbE"
      },
      "source": [
        "Preparamos los features para el modelo, es decir, seteamos cada una de las columnas que vayamos a utilizar del DataFrame. Luego generamos el DataSet en base al DataFrame para darselo como input al modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "7bTkNirdLlCr",
        "outputId": "16fc3bae-fb17-40d2-ec0e-a6396cf6ced1"
      },
      "source": [
        "  #Columnas que consideramos numericas\r\n",
        "  numeric_columns = ['Total_Taxable_Amount','Product_Amount','ASP_converted','TRF']\r\n",
        "\r\n",
        "  #Columnas que consideramos clasificatorias con rango numerico\r\n",
        "  bucket_columns = ['Delta_Time']\r\n",
        "\r\n",
        "  #Columnas que queremos hashear\r\n",
        "  hashed_columns = []#'Products']\r\n",
        "\r\n",
        "  #Columnas que consideramos categoricas de pocos valores posibles\r\n",
        "  indicator_columns = ['Region', \r\n",
        "                       'Source', \r\n",
        "                       'Account_Type',\r\n",
        "                       'Opportunity_Type',\r\n",
        "                       'Delivery_Quarter',\r\n",
        "                       'Has_Expiry_Date',\r\n",
        "                       'Brand',\r\n",
        "                       'Same_Owner', 'Same_Owner_Modifier'\r\n",
        "                       ]\r\n",
        "                        #'Last_Modified_By'\r\n",
        "                        #'Product_Family', 'Product_Name',\r\n",
        "                        #'Account_Owner', 'Opportunity_Owner', 'Account_Name'\r\n",
        "                        #'Territory', 'Billing_Country'\r\n",
        "\r\n",
        "  #Columnas crossed, aquellas que queremos un parametro por combinacion\r\n",
        "  crossed_columns = [['Bureaucratic_Code','Bureaucratic_Code_0_Approval',\r\n",
        "                      'Bureaucratic_Code_0_Approved'],\r\n",
        "                     ['Delivery_Terms',\r\n",
        "                      'Pricing_Delivery_Terms_Quote_Appr',\r\n",
        "                      'Pricing_Delivery_Terms_Approved']]#['Brand', 'Has_Contract_Number'],#,'Has_Expiry_Date'],\r\n",
        "                     #[],\r\n",
        "                     #['Bureaucratic_Code','Bureaucratic_Code_Approved']]\r\n",
        "                     #['Has_Contract_Number','Delivery_Quarter']]\r\n",
        "\r\n",
        "  df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "  df = preprocess_dataframe(df)\r\n",
        "\r\n",
        "  features = set_up_feature_columns(df,numeric_columns,indicator_columns,bucket_columns, crossed_columns, hashed_columns)\r\n",
        "\r\n",
        "  #Separamos el DataFrame en uno de prueba, validacion y entrenamiento\r\n",
        "  test_lines = 400\r\n",
        "\r\n",
        "  np.random.seed(1)\r\n",
        "  drop_indices = np.random.choice(df.index, test_lines, replace=False)\r\n",
        "  df_test = df.loc[drop_indices, :]\r\n",
        "  df.drop(drop_indices, inplace=True)\r\n",
        "\r\n",
        "  #df_validation = df.tail(200)\r\n",
        "  #df.drop(df.tail(200).index, inplace=True)\r\n",
        "\r\n",
        "  feature_layer = tf.keras.layers.DenseFeatures(features)\r\n",
        "  ds = df_to_dataset(df,batch_size=56, shuffle=False)\r\n",
        "  ds_test = df_to_dataset(df_test, batch_size=56, shuffle=False)\r\n",
        "  #ds_validation = df_to_dataset(df_validation, batch_size=56, shuffle=False)\r\n",
        "\r\n",
        "  df"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4582: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region</th>\n",
              "      <th>Pricing_Delivery_Terms_Quote_Appr</th>\n",
              "      <th>Pricing_Delivery_Terms_Approved</th>\n",
              "      <th>Bureaucratic_Code_0_Approval</th>\n",
              "      <th>Bureaucratic_Code_0_Approved</th>\n",
              "      <th>Bureaucratic_Code</th>\n",
              "      <th>Source</th>\n",
              "      <th>Account_Type</th>\n",
              "      <th>Opportunity_Type</th>\n",
              "      <th>Quote_Type</th>\n",
              "      <th>Delivery_Terms</th>\n",
              "      <th>Brand</th>\n",
              "      <th>Has_Expiry_Date</th>\n",
              "      <th>ASP_converted</th>\n",
              "      <th>Delivery_Quarter</th>\n",
              "      <th>TRF</th>\n",
              "      <th>Total_Taxable_Amount</th>\n",
              "      <th>Stage</th>\n",
              "      <th>Delta_Time</th>\n",
              "      <th>Product_Amount</th>\n",
              "      <th>Same_Owner</th>\n",
              "      <th>Same_Owner_Modifier</th>\n",
              "      <th>Delivery_Approved</th>\n",
              "      <th>Bureaucratic_Code_Approved</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EMEA</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>None</td>\n",
              "      <td>Account_Type_2</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_2</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.215056</td>\n",
              "      <td>Q2</td>\n",
              "      <td>0.482815</td>\n",
              "      <td>0.547199</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EMEA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>None</td>\n",
              "      <td>Account_Type_2</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_2</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.227577</td>\n",
              "      <td>Q1</td>\n",
              "      <td>-0.204252</td>\n",
              "      <td>-0.172044</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Americas</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>Source_7</td>\n",
              "      <td>Account_Type_5</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_4</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.095303</td>\n",
              "      <td>Q1</td>\n",
              "      <td>-0.204252</td>\n",
              "      <td>-0.168476</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Americas</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_5</td>\n",
              "      <td>Source_11</td>\n",
              "      <td>Account_Type_5</td>\n",
              "      <td>Opportunity_Type_19</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>0.150657</td>\n",
              "      <td>Q1</td>\n",
              "      <td>0.757642</td>\n",
              "      <td>0.724658</td>\n",
              "      <td>0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Americas</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_5</td>\n",
              "      <td>Source_11</td>\n",
              "      <td>Account_Type_5</td>\n",
              "      <td>Opportunity_Type_19</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.150657</td>\n",
              "      <td>Q1</td>\n",
              "      <td>1.513417</td>\n",
              "      <td>1.447064</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16939</th>\n",
              "      <td>EMEA</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>Source_9</td>\n",
              "      <td>Account_Type_0</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_2</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.208790</td>\n",
              "      <td>Q2</td>\n",
              "      <td>-0.135546</td>\n",
              "      <td>-0.123383</td>\n",
              "      <td>1</td>\n",
              "      <td>20.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16940</th>\n",
              "      <td>Americas</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Bureaucratic_Code_5</td>\n",
              "      <td>None</td>\n",
              "      <td>Account_Type_5</td>\n",
              "      <td>Opportunity_Type_19</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_4</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.150657</td>\n",
              "      <td>Q4</td>\n",
              "      <td>1.169883</td>\n",
              "      <td>2.417765</td>\n",
              "      <td>0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.162891</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16942</th>\n",
              "      <td>EMEA</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>Source_7</td>\n",
              "      <td>Account_Type_5</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_2</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.215056</td>\n",
              "      <td>Q1</td>\n",
              "      <td>-0.204252</td>\n",
              "      <td>-0.137423</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.753520</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16945</th>\n",
              "      <td>Americas</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>None</td>\n",
              "      <td>Account_Type_5</td>\n",
              "      <td>Opportunity_Type_1</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_4</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.269669</td>\n",
              "      <td>Q2</td>\n",
              "      <td>0.070575</td>\n",
              "      <td>-0.178684</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16946</th>\n",
              "      <td>Americas</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Bureaucratic_Code_4</td>\n",
              "      <td>None</td>\n",
              "      <td>Account_Type_5</td>\n",
              "      <td>Opportunity_Type_19</td>\n",
              "      <td>Non Binding</td>\n",
              "      <td>Delivery_Terms_4</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.272437</td>\n",
              "      <td>Q3</td>\n",
              "      <td>2.544018</td>\n",
              "      <td>-0.178684</td>\n",
              "      <td>0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>-0.427738</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9390 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Region  ...  Bureaucratic_Code_Approved\n",
              "0          EMEA  ...                           1\n",
              "1          EMEA  ...                           1\n",
              "2      Americas  ...                           1\n",
              "3      Americas  ...                           0\n",
              "4      Americas  ...                           0\n",
              "...         ...  ...                         ...\n",
              "16939      EMEA  ...                           1\n",
              "16940  Americas  ...                           1\n",
              "16942      EMEA  ...                           1\n",
              "16945  Americas  ...                           1\n",
              "16946  Americas  ...                           1\n",
              "\n",
              "[9390 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgLC9fhu-Mqu"
      },
      "source": [
        "Creamos y compilamos el modelo. En esta seccion se tunean las propiedades del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgXH6i3kL9ZN"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "  feature_layer,\r\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgHkvYWxP2h"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stA0Y5lI9-Jy"
      },
      "source": [
        "Entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6px_EtjoYRXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51ad301-1fd6-42fe-f7f7-5ff6fb36462f"
      },
      "source": [
        "#Ignoren el WARNING, esta en la documentacion tambien. Nadie le da bola en StackOverflow xd.\r\n",
        "model.fit(ds, validation_data=ds_test, epochs=110)\r\n",
        "#model.fit(ds, epochs=60)\r\n",
        "model.summary()"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/110\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Region': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=string>, 'Pricing_Delivery_Terms_Quote_Appr': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=int64>, 'Pricing_Delivery_Terms_Approved': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approval': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approved': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Quote_Type': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Has_Expiry_Date': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'ASP_converted': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'Delivery_Quarter': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'TRF': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'Total_Taxable_Amount': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'Delta_Time': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, 'Product_Amount': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, 'Same_Owner': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=string>, 'Same_Owner_Modifier': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=string>, 'Delivery_Approved': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_Approved': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=int64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Region': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=string>, 'Pricing_Delivery_Terms_Quote_Appr': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=int64>, 'Pricing_Delivery_Terms_Approved': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approval': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approved': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Quote_Type': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Has_Expiry_Date': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'ASP_converted': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'Delivery_Quarter': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'TRF': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'Total_Taxable_Amount': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'Delta_Time': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, 'Product_Amount': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, 'Same_Owner': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=string>, 'Same_Owner_Modifier': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=string>, 'Delivery_Approved': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_Approved': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=int64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "160/168 [===========================>..] - ETA: 0s - loss: 0.6873 - accuracy: 0.5169WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Region': <tf.Tensor 'ExpandDims_17:0' shape=(None, 1) dtype=string>, 'Pricing_Delivery_Terms_Quote_Appr': <tf.Tensor 'ExpandDims_14:0' shape=(None, 1) dtype=int64>, 'Pricing_Delivery_Terms_Approved': <tf.Tensor 'ExpandDims_13:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approval': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_0_Approved': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Source': <tf.Tensor 'ExpandDims_20:0' shape=(None, 1) dtype=string>, 'Account_Type': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Opportunity_Type': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Quote_Type': <tf.Tensor 'ExpandDims_16:0' shape=(None, 1) dtype=string>, 'Delivery_Terms': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=string>, 'Brand': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Has_Expiry_Date': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'ASP_converted': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float64>, 'Delivery_Quarter': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'TRF': <tf.Tensor 'ExpandDims_21:0' shape=(None, 1) dtype=float64>, 'Total_Taxable_Amount': <tf.Tensor 'ExpandDims_22:0' shape=(None, 1) dtype=float64>, 'Delta_Time': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=float64>, 'Product_Amount': <tf.Tensor 'ExpandDims_15:0' shape=(None, 1) dtype=float64>, 'Same_Owner': <tf.Tensor 'ExpandDims_18:0' shape=(None, 1) dtype=string>, 'Same_Owner_Modifier': <tf.Tensor 'ExpandDims_19:0' shape=(None, 1) dtype=string>, 'Delivery_Approved': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=int64>, 'Bureaucratic_Code_Approved': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=int64>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "168/168 [==============================] - 2s 9ms/step - loss: 0.6858 - accuracy: 0.5232 - val_loss: 0.6351 - val_accuracy: 0.6650\n",
            "Epoch 2/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.6115 - accuracy: 0.7091 - val_loss: 0.5972 - val_accuracy: 0.6950\n",
            "Epoch 3/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.5694 - accuracy: 0.7313 - val_loss: 0.5736 - val_accuracy: 0.7050\n",
            "Epoch 4/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.5434 - accuracy: 0.7383 - val_loss: 0.5577 - val_accuracy: 0.7175\n",
            "Epoch 5/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.5262 - accuracy: 0.7462 - val_loss: 0.5461 - val_accuracy: 0.7200\n",
            "Epoch 6/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.5140 - accuracy: 0.7488 - val_loss: 0.5371 - val_accuracy: 0.7250\n",
            "Epoch 7/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.5051 - accuracy: 0.7588 - val_loss: 0.5299 - val_accuracy: 0.7200\n",
            "Epoch 8/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4983 - accuracy: 0.7634 - val_loss: 0.5239 - val_accuracy: 0.7300\n",
            "Epoch 9/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4929 - accuracy: 0.7642 - val_loss: 0.5187 - val_accuracy: 0.7375\n",
            "Epoch 10/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4885 - accuracy: 0.7675 - val_loss: 0.5142 - val_accuracy: 0.7400\n",
            "Epoch 11/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4849 - accuracy: 0.7697 - val_loss: 0.5103 - val_accuracy: 0.7400\n",
            "Epoch 12/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4819 - accuracy: 0.7702 - val_loss: 0.5067 - val_accuracy: 0.7375\n",
            "Epoch 13/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4793 - accuracy: 0.7704 - val_loss: 0.5035 - val_accuracy: 0.7400\n",
            "Epoch 14/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4771 - accuracy: 0.7705 - val_loss: 0.5006 - val_accuracy: 0.7450\n",
            "Epoch 15/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4751 - accuracy: 0.7704 - val_loss: 0.4980 - val_accuracy: 0.7450\n",
            "Epoch 16/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4734 - accuracy: 0.7709 - val_loss: 0.4956 - val_accuracy: 0.7475\n",
            "Epoch 17/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4718 - accuracy: 0.7711 - val_loss: 0.4934 - val_accuracy: 0.7475\n",
            "Epoch 18/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4705 - accuracy: 0.7732 - val_loss: 0.4914 - val_accuracy: 0.7475\n",
            "Epoch 19/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4692 - accuracy: 0.7751 - val_loss: 0.4895 - val_accuracy: 0.7500\n",
            "Epoch 20/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4681 - accuracy: 0.7753 - val_loss: 0.4877 - val_accuracy: 0.7500\n",
            "Epoch 21/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4671 - accuracy: 0.7787 - val_loss: 0.4861 - val_accuracy: 0.7600\n",
            "Epoch 22/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4662 - accuracy: 0.7807 - val_loss: 0.4846 - val_accuracy: 0.7600\n",
            "Epoch 23/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4653 - accuracy: 0.7807 - val_loss: 0.4832 - val_accuracy: 0.7600\n",
            "Epoch 24/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4645 - accuracy: 0.7820 - val_loss: 0.4819 - val_accuracy: 0.7600\n",
            "Epoch 25/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4638 - accuracy: 0.7815 - val_loss: 0.4807 - val_accuracy: 0.7625\n",
            "Epoch 26/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4631 - accuracy: 0.7842 - val_loss: 0.4795 - val_accuracy: 0.7625\n",
            "Epoch 27/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4624 - accuracy: 0.7840 - val_loss: 0.4784 - val_accuracy: 0.7625\n",
            "Epoch 28/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4618 - accuracy: 0.7853 - val_loss: 0.4774 - val_accuracy: 0.7650\n",
            "Epoch 29/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4613 - accuracy: 0.7867 - val_loss: 0.4765 - val_accuracy: 0.7650\n",
            "Epoch 30/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4607 - accuracy: 0.7874 - val_loss: 0.4755 - val_accuracy: 0.7675\n",
            "Epoch 31/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4602 - accuracy: 0.7880 - val_loss: 0.4747 - val_accuracy: 0.7675\n",
            "Epoch 32/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4598 - accuracy: 0.7881 - val_loss: 0.4739 - val_accuracy: 0.7675\n",
            "Epoch 33/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4593 - accuracy: 0.7883 - val_loss: 0.4731 - val_accuracy: 0.7675\n",
            "Epoch 34/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4589 - accuracy: 0.7887 - val_loss: 0.4724 - val_accuracy: 0.7675\n",
            "Epoch 35/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4585 - accuracy: 0.7887 - val_loss: 0.4717 - val_accuracy: 0.7675\n",
            "Epoch 36/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4581 - accuracy: 0.7912 - val_loss: 0.4711 - val_accuracy: 0.7650\n",
            "Epoch 37/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4578 - accuracy: 0.7905 - val_loss: 0.4705 - val_accuracy: 0.7675\n",
            "Epoch 38/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4574 - accuracy: 0.7910 - val_loss: 0.4699 - val_accuracy: 0.7675\n",
            "Epoch 39/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4571 - accuracy: 0.7916 - val_loss: 0.4693 - val_accuracy: 0.7675\n",
            "Epoch 40/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4568 - accuracy: 0.7917 - val_loss: 0.4688 - val_accuracy: 0.7750\n",
            "Epoch 41/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4565 - accuracy: 0.7914 - val_loss: 0.4683 - val_accuracy: 0.7750\n",
            "Epoch 42/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4562 - accuracy: 0.7914 - val_loss: 0.4678 - val_accuracy: 0.7725\n",
            "Epoch 43/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4559 - accuracy: 0.7913 - val_loss: 0.4673 - val_accuracy: 0.7725\n",
            "Epoch 44/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4556 - accuracy: 0.7912 - val_loss: 0.4669 - val_accuracy: 0.7725\n",
            "Epoch 45/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4554 - accuracy: 0.7916 - val_loss: 0.4665 - val_accuracy: 0.7725\n",
            "Epoch 46/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4551 - accuracy: 0.7919 - val_loss: 0.4661 - val_accuracy: 0.7725\n",
            "Epoch 47/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4549 - accuracy: 0.7923 - val_loss: 0.4657 - val_accuracy: 0.7725\n",
            "Epoch 48/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4547 - accuracy: 0.7925 - val_loss: 0.4653 - val_accuracy: 0.7725\n",
            "Epoch 49/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4545 - accuracy: 0.7925 - val_loss: 0.4650 - val_accuracy: 0.7725\n",
            "Epoch 50/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4543 - accuracy: 0.7932 - val_loss: 0.4646 - val_accuracy: 0.7725\n",
            "Epoch 51/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4541 - accuracy: 0.7930 - val_loss: 0.4643 - val_accuracy: 0.7725\n",
            "Epoch 52/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4539 - accuracy: 0.7927 - val_loss: 0.4640 - val_accuracy: 0.7725\n",
            "Epoch 53/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4537 - accuracy: 0.7929 - val_loss: 0.4637 - val_accuracy: 0.7725\n",
            "Epoch 54/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4535 - accuracy: 0.7927 - val_loss: 0.4634 - val_accuracy: 0.7725\n",
            "Epoch 55/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4533 - accuracy: 0.7930 - val_loss: 0.4632 - val_accuracy: 0.7725\n",
            "Epoch 56/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4532 - accuracy: 0.7933 - val_loss: 0.4629 - val_accuracy: 0.7725\n",
            "Epoch 57/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4530 - accuracy: 0.7936 - val_loss: 0.4626 - val_accuracy: 0.7725\n",
            "Epoch 58/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4529 - accuracy: 0.7936 - val_loss: 0.4624 - val_accuracy: 0.7725\n",
            "Epoch 59/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4527 - accuracy: 0.7938 - val_loss: 0.4621 - val_accuracy: 0.7725\n",
            "Epoch 60/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4526 - accuracy: 0.7940 - val_loss: 0.4619 - val_accuracy: 0.7725\n",
            "Epoch 61/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4525 - accuracy: 0.7934 - val_loss: 0.4617 - val_accuracy: 0.7725\n",
            "Epoch 62/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4523 - accuracy: 0.7932 - val_loss: 0.4615 - val_accuracy: 0.7725\n",
            "Epoch 63/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4522 - accuracy: 0.7935 - val_loss: 0.4613 - val_accuracy: 0.7700\n",
            "Epoch 64/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4521 - accuracy: 0.7938 - val_loss: 0.4611 - val_accuracy: 0.7700\n",
            "Epoch 65/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4520 - accuracy: 0.7935 - val_loss: 0.4609 - val_accuracy: 0.7700\n",
            "Epoch 66/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4519 - accuracy: 0.7932 - val_loss: 0.4607 - val_accuracy: 0.7700\n",
            "Epoch 67/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4517 - accuracy: 0.7941 - val_loss: 0.4605 - val_accuracy: 0.7700\n",
            "Epoch 68/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4516 - accuracy: 0.7941 - val_loss: 0.4604 - val_accuracy: 0.7700\n",
            "Epoch 69/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4515 - accuracy: 0.7941 - val_loss: 0.4602 - val_accuracy: 0.7700\n",
            "Epoch 70/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4514 - accuracy: 0.7943 - val_loss: 0.4600 - val_accuracy: 0.7725\n",
            "Epoch 71/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4513 - accuracy: 0.7944 - val_loss: 0.4599 - val_accuracy: 0.7725\n",
            "Epoch 72/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4512 - accuracy: 0.7945 - val_loss: 0.4597 - val_accuracy: 0.7700\n",
            "Epoch 73/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4511 - accuracy: 0.7945 - val_loss: 0.4596 - val_accuracy: 0.7700\n",
            "Epoch 74/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4510 - accuracy: 0.7946 - val_loss: 0.4594 - val_accuracy: 0.7700\n",
            "Epoch 75/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4510 - accuracy: 0.7948 - val_loss: 0.4593 - val_accuracy: 0.7700\n",
            "Epoch 76/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4509 - accuracy: 0.7947 - val_loss: 0.4592 - val_accuracy: 0.7700\n",
            "Epoch 77/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4508 - accuracy: 0.7950 - val_loss: 0.4591 - val_accuracy: 0.7700\n",
            "Epoch 78/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4507 - accuracy: 0.7956 - val_loss: 0.4589 - val_accuracy: 0.7700\n",
            "Epoch 79/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4506 - accuracy: 0.7957 - val_loss: 0.4588 - val_accuracy: 0.7700\n",
            "Epoch 80/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4505 - accuracy: 0.7956 - val_loss: 0.4587 - val_accuracy: 0.7700\n",
            "Epoch 81/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4505 - accuracy: 0.7956 - val_loss: 0.4586 - val_accuracy: 0.7725\n",
            "Epoch 82/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4504 - accuracy: 0.7956 - val_loss: 0.4585 - val_accuracy: 0.7725\n",
            "Epoch 83/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4503 - accuracy: 0.7954 - val_loss: 0.4584 - val_accuracy: 0.7725\n",
            "Epoch 84/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4503 - accuracy: 0.7955 - val_loss: 0.4583 - val_accuracy: 0.7750\n",
            "Epoch 85/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4502 - accuracy: 0.7953 - val_loss: 0.4582 - val_accuracy: 0.7750\n",
            "Epoch 86/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4501 - accuracy: 0.7955 - val_loss: 0.4581 - val_accuracy: 0.7750\n",
            "Epoch 87/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4500 - accuracy: 0.7954 - val_loss: 0.4580 - val_accuracy: 0.7750\n",
            "Epoch 88/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4500 - accuracy: 0.7955 - val_loss: 0.4579 - val_accuracy: 0.7750\n",
            "Epoch 89/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4499 - accuracy: 0.7958 - val_loss: 0.4578 - val_accuracy: 0.7750\n",
            "Epoch 90/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4499 - accuracy: 0.7961 - val_loss: 0.4577 - val_accuracy: 0.7750\n",
            "Epoch 91/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4498 - accuracy: 0.7962 - val_loss: 0.4576 - val_accuracy: 0.7750\n",
            "Epoch 92/110\n",
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4497 - accuracy: 0.7962 - val_loss: 0.4576 - val_accuracy: 0.7750\n",
            "Epoch 93/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4497 - accuracy: 0.7964 - val_loss: 0.4575 - val_accuracy: 0.7750\n",
            "Epoch 94/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4496 - accuracy: 0.7962 - val_loss: 0.4574 - val_accuracy: 0.7750\n",
            "Epoch 95/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4496 - accuracy: 0.7962 - val_loss: 0.4573 - val_accuracy: 0.7750\n",
            "Epoch 96/110\n",
            "168/168 [==============================] - 1s 8ms/step - loss: 0.4495 - accuracy: 0.7962 - val_loss: 0.4573 - val_accuracy: 0.7750\n",
            "Epoch 97/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4495 - accuracy: 0.7962 - val_loss: 0.4572 - val_accuracy: 0.7750\n",
            "Epoch 98/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4494 - accuracy: 0.7958 - val_loss: 0.4571 - val_accuracy: 0.7750\n",
            "Epoch 99/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4493 - accuracy: 0.7957 - val_loss: 0.4571 - val_accuracy: 0.7750\n",
            "Epoch 100/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4493 - accuracy: 0.7955 - val_loss: 0.4570 - val_accuracy: 0.7750\n",
            "Epoch 101/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4492 - accuracy: 0.7955 - val_loss: 0.4570 - val_accuracy: 0.7750\n",
            "Epoch 102/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4492 - accuracy: 0.7955 - val_loss: 0.4569 - val_accuracy: 0.7750\n",
            "Epoch 103/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4492 - accuracy: 0.7955 - val_loss: 0.4568 - val_accuracy: 0.7750\n",
            "Epoch 104/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4491 - accuracy: 0.7956 - val_loss: 0.4568 - val_accuracy: 0.7750\n",
            "Epoch 105/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4491 - accuracy: 0.7956 - val_loss: 0.4567 - val_accuracy: 0.7750\n",
            "Epoch 106/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4490 - accuracy: 0.7956 - val_loss: 0.4567 - val_accuracy: 0.7750\n",
            "Epoch 107/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4490 - accuracy: 0.7956 - val_loss: 0.4566 - val_accuracy: 0.7725\n",
            "Epoch 108/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4489 - accuracy: 0.7957 - val_loss: 0.4566 - val_accuracy: 0.7725\n",
            "Epoch 109/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4489 - accuracy: 0.7958 - val_loss: 0.4565 - val_accuracy: 0.7725\n",
            "Epoch 110/110\n",
            "168/168 [==============================] - 1s 7ms/step - loss: 0.4488 - accuracy: 0.7957 - val_loss: 0.4565 - val_accuracy: 0.7725\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_features_20 (DenseFeat multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             multiple                  746       \n",
            "=================================================================\n",
            "Total params: 746\n",
            "Trainable params: 746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYTFqZSM-TA-"
      },
      "source": [
        "Evaluamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SDVpkRXMmLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70063008-fa3b-447b-a895-810766fc5a1a"
      },
      "source": [
        "model.evaluate(ds)"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "168/168 [==============================] - 1s 6ms/step - loss: 0.4576 - accuracy: 0.7809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4575735628604889, 0.7809371948242188]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1z7YhHrAGLc"
      },
      "source": [
        "Escribimos las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8BZJqotAFqU"
      },
      "source": [
        "'''\r\n",
        "frio_test_df = pd.read_csv('/content/Test_TP2_Datos_2020-2C.csv')\r\n",
        "frio_test_df['Stage'] = 'Closed Won' #Esto esta solo para que funque todo, no lo uso. No se bien como armarlo sin los labels de Stage. TODO: Averiguar como es!\r\n",
        "aux_df = frio_test_df[['Opportunity_ID']] #Esta columna la vuela el preprocesado sino\r\n",
        "frio_test_df = preprocess_dataframe(frio_test_df)\r\n",
        "frio_test_ds = df_to_dataset(frio_test_df, shuffle=False, batch_size=56)\r\n",
        "predictions = model.predict(frio_test_ds)\r\n",
        "\r\n",
        "aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True) #Lo hacia el preprocesado pero es verdad que lo copie antes a este xd, perdon Agus, paja de dejarlo lindo.\r\n",
        "aux_df['Target'] = predictions\r\n",
        "\r\n",
        "aux_df.to_csv('prediccionesFrioFrio.csv', index=False)\r\n",
        "'''\r\n",
        "df = pd.read_csv('/content/Train_TP2_Datos_2020-2C.csv')\r\n",
        "df = df[(df['Stage'] == 'Closed Won') | (df['Stage'] == 'Closed Lost')]\r\n",
        "df = df[df['Opportunity_ID'] != 9773]\r\n",
        "df = df[['Opportunity_ID']]\r\n",
        "df.drop_duplicates(subset='Opportunity_ID', inplace=True)\r\n",
        "np.random.seed(1)\r\n",
        "drop_indices = np.random.choice(df.index, test_lines, replace=False)\r\n",
        "df.drop(drop_indices, inplace=True)\r\n",
        "predictions = model.predict(ds)\r\n",
        "df['Target'] = predictions\r\n",
        "df.to_csv('prediccionesFrioFrio.csv', index=False)\r\n"
      ],
      "execution_count": 264,
      "outputs": []
    }
  ]
}