{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "from sklearn import metrics\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "\n",
    "  df.fillna(value=0, inplace=True) #Reemplazamos NAN por 0, ya que NAN rompe a Tensorflow\n",
    "\n",
    "  #Renombramos las columnas que tienen caracteres que TensorFlow no acepta como validos.\n",
    "  #Estos particularmente son whitespace, coma y parentesis por ejemplo.\n",
    "  df.rename(columns={'ASP_(converted)':'ASP_converted','Pricing, Delivery_Terms_Quote_Appr':\n",
    "                    'Pricing_Delivery_Terms_Quote_Appr','Pricing, Delivery_Terms_Approved':\n",
    "                    'Pricing_Delivery_Terms_Approved','Source ':'Source'},inplace=True)\n",
    "\n",
    "  df = df[df['Stage'].isin(['Closed Won', 'Closed Lost'])]\n",
    "  df.loc[:, 'Stage'].replace({'Closed Won':1, 'Closed Lost':0}, inplace=True) \n",
    "#0 corresponde a que el caso fue Closed Lost, 1 a que fue Closed Won. Asi tenemos un problema de clasificacion binario que puede entender la red neuronal.\n",
    "\n",
    "  df.loc[:, 'Planned_Delivery_Start_Date'] = pd.to_datetime(df['Planned_Delivery_Start_Date'], 'coerce',\n",
    "                                                                  format='%m/%d/%Y')\n",
    "  df.loc[:, 'Planned_Delivery_End_Date'] = pd.to_datetime(df['Planned_Delivery_End_Date'], 'coerce',\n",
    "                                                                                      format='%m/%d/%Y')\n",
    "  df.loc[:, 'Last_Modified_Date'] = pd.to_datetime(df['Last_Modified_Date'], 'coerce',\n",
    "                                                                                      format='%m/%d/%Y')\n",
    "  df.loc[:, 'Quote_Expiry_Date'] = pd.to_datetime(df['Quote_Expiry_Date'], 'coerce', format='%m/%d/%Y')\n",
    "\n",
    "  df.loc[:, 'Opportunity_Created_Date'] = pd.to_datetime(df['Opportunity_Created_Date'], 'coerce',\n",
    "                                                                                      format='%m/%d/%Y')\n",
    "  df.loc[:, 'Account_Created_Date'] = pd.to_datetime(df['Account_Created_Date'], 'coerce', format='%m/%d/%Y')\n",
    "\n",
    "  df = df[df['Opportunity_ID'] != 9773] #Hardcodeo este filtrado porque el id 9773 tiene mal cargada la fecha de delivery end, dando una diferencia de 200 anios xd\"\n",
    "\n",
    "  #Pongo .loc porque pandas me jode con warnings que son falsos positivos de slice copy\"\n",
    "  #Gracias Pandas!\"\n",
    "\n",
    "  #Creamos una nueva columna (Feature Engineering) que contiene la longitud en dias \n",
    "  #estimada de la operacion. En el informe habiamos encontrado que aparentaba haber\n",
    "  #una relacion cuadratica de decrecimiento a medida que aumentaban los dias donde disminuia\n",
    "  #la chance de completar la operacion.\n",
    "  df['Planned_Opportunity_Time'] = df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']\n",
    "  df.loc[:, 'Planned_Opportunity_Time'] = df['Planned_Opportunity_Time'].dt.days\n",
    "  df['Planned_Opportunity_Time'] = df['Planned_Opportunity_Time'].replace({np.nan:10.0}) #Reemplazo con 10 porque los que no tienen fecha final ganan el 60%, y el analisis de los datos da que el 60% es maso a los 10 dias. Asi no jodo el resto de los datos\n",
    "  df['Planned_Opportunity_Time'] = df.groupby('Opportunity_ID')['Planned_Opportunity_Time'].transform('max')\n",
    "\n",
    "  df['End_Date_vs_Modified_Date_Difference'] = df['Planned_Delivery_End_Date'] - df['Last_Modified_Date']\n",
    "  df.loc[:, 'End_Date_vs_Modified_Date_Difference'] = df['End_Date_vs_Modified_Date_Difference'].dt.days\n",
    "  df['End_Date_vs_Modified_Date_Difference'] = df['End_Date_vs_Modified_Date_Difference'].replace({np.nan:10.0})\n",
    "  df['End_Date_vs_Modified_Date_Difference'] = df.groupby('Opportunity_ID')['End_Date_vs_Modified_Date_Difference'].transform('max')\n",
    "\n",
    "  df['Expiry_Date_vs_Modified_Date_Difference'] = df['Planned_Delivery_End_Date'] - df['Quote_Expiry_Date']\n",
    "  df.loc[:, 'Expiry_Date_vs_Modified_Date_Difference'] = df['Expiry_Date_vs_Modified_Date_Difference'].dt.days\n",
    "  df['Expiry_Date_vs_Modified_Date_Difference'] = df['Expiry_Date_vs_Modified_Date_Difference'].replace({np.nan:10.0})\n",
    "  df['Expiry_Date_vs_Modified_Date_Difference'] = df.groupby('Opportunity_ID')['Expiry_Date_vs_Modified_Date_Difference'].transform('max')\n",
    "\n",
    "  df['Acc_vs_Opp_Creation_Dates'] = df['Opportunity_Created_Date'] - df['Account_Created_Date']\n",
    "  df.loc[:, 'Acc_vs_Opp_Creation_Dates'] = df['Acc_vs_Opp_Creation_Dates'].dt.days\n",
    "  df['Expiry_Date_vs_Modified_Date_Difference'] = df['Acc_vs_Opp_Creation_Dates'].replace({np.nan:10})\n",
    "  df['Acc_vs_Opp_Creation_Dates'] = df.groupby('Opportunity_ID')['Acc_vs_Opp_Creation_Dates'].transform('max')\n",
    "\n",
    "  #Agrego una columna que indique la cantidad de productos que tiene esa\n",
    "  #oportunidad\n",
    "  df['Product'] = 1\n",
    "  df['Product_Amount'] = df.groupby('Opportunity_ID')['Product'].transform(lambda x: x.sum())\n",
    "\n",
    "\n",
    "  #Pasamos todo a dolares\n",
    "  currency_conversion = {'AUD':0.707612, 'EUR':1.131064, 'GBP':1.318055, 'JPY':0.008987, 'USD':1.0}\n",
    "  df['Total_Taxable_Amount_Currency'] = df[['Total_Taxable_Amount_Currency']].replace(currency_conversion)\n",
    "  df['Total_Taxable_Amount'] = df['Total_Taxable_Amount_Currency'] * df['Total_Taxable_Amount']\n",
    "  #df['Total_Taxable_Amount'] = df.groupby(\"Opportunity_ID\")['Total_Taxable_Amount'].transform(\"sum\")\n",
    "  df['Total_Taxable_Amount'] = df['Total_Taxable_Amount'] * df['Product_Amount']\n",
    "\n",
    "  #Modifico la columna Brand para que en vez de decir que marca es, solo diga\n",
    "  #si tiene o no marca. Es importante aclarar que verificamos que siempre que una oportunidad\n",
    "  #tiene un producto con marca entonces todos sus productos tienen marca. Esto se cumple\n",
    "  #tanto en el set de entrenamiento como en el de test, por lo tanto al hacer drop_duplicates\n",
    "  #no nos va a pasar nunca el caso donde nos pudieramos quedar con una entrada de producto\n",
    "  #sin marca mientras que algun otro producto si tuviera, ya que confirmamos que o todos tienen\n",
    "  #marca o ninguno tiene.\n",
    "  df.loc[df['Brand'] == 'None', 'Brand'] = 'No'\n",
    "  df.loc[df['Brand'] != 'No', 'Brand'] = 'Yes'\n",
    "\n",
    "  #Agrego una columna que indica si el owner de la cuenta es el mismo que el de la oportunidad\n",
    "  #o no\n",
    "  df['Same_Owner'] = (df['Account_Owner'] == df['Opportunity_Owner'])\n",
    "  df['Same_Owner'] = df['Same_Owner'].replace({False:'No', True:'Yes'})\n",
    "\n",
    "  #Agrego columna que indica si el ultimo que modifico la oportunidad es el mismo que el opportunity owner\n",
    "  df['Same_Owner_Modifier'] = (df['Last_Modified_By'] == df['Opportunity_Owner'])\n",
    "  df['Same_Owner_Modifier'] = df['Same_Owner_Modifier'].replace({False:'No', True:'Yes'})\n",
    "\n",
    "  #Agrego una columna que indica si tiene o no fecha de expiracion\n",
    "  df['Quote_Expiry_Date'] = (df['Quote_Expiry_Date'] != 'NaT')\n",
    "  df.rename(columns={'Quote_Expiry_Date':'Has_Expiry_Date'}, inplace=True)\n",
    "  df['Has_Expiry_Date'] = df['Has_Expiry_Date'].replace({True:'Yes',False:'No'})\n",
    "\n",
    "  #Reemplazo las 4 columnas de aprobacion por solo 2 columnas que indiquen si tuvo la aprobacion\n",
    "  #de delivery y burocratica o no. Recalco que si nunca la necesito seria equivalente a si\n",
    "  #la necesito y la consiguio.\n",
    "  df['Delivery_Approved'] = df['Pricing_Delivery_Terms_Quote_Appr'] + df['Pricing_Delivery_Terms_Approved']\n",
    "  df['Delivery_Approved'] = df['Delivery_Approved'].replace({0:1, 1:0, 2:1})\n",
    "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_0_Approval'] + df['Bureaucratic_Code_0_Approved']\n",
    "  df['Bureaucratic_Code_Approved'] = df['Bureaucratic_Code_Approved'].replace({0:1, 1:0, 2:1})\n",
    "  df['Approved'] = df['Delivery_Approved'] & df['Bureaucratic_Code_Approved']\n",
    "\n",
    "  df['Territory_Is_None'] = df['Territory'] == \"None\"\n",
    "\n",
    "  #Cambio TRF por una columna que es el valor maximo de los TRF de la oportunidad\n",
    "  df[\"TRF\"] = df.groupby(\"Opportunity_ID\")[\"TRF\"].transform(\"max\")\n",
    "\n",
    "  def combineProducts(x):\n",
    "    products = \"\"\n",
    "    added = []\n",
    "    for product in x:\n",
    "      product = re.findall('\\d+', product)[0]\n",
    "      if added.count(product) == 0:\n",
    "        products += product\n",
    "        added.append(product)\n",
    "    return products\n",
    "\n",
    "  #Junto todos los productos en una sola entrada\n",
    "  df['Products'] = df.groupby('Opportunity_ID')['Product_Family'].transform(combineProducts)\n",
    "\n",
    "  #Pruebo volar duplicados, solo cambia el producto. Si el producto no importa\n",
    "  #entonces volar duplicados no deberia importar. Obviamente vuelo el producto en el que\n",
    "  #quede tambien.\n",
    "  df.drop_duplicates('Opportunity_Name',inplace=True)\n",
    "  df.drop(columns=['Product_Name','Product_Family','Opportunity_Name'],inplace=True)\n",
    "\n",
    "  #Normalizo las columnas numericas\n",
    "  normalized_columns = ['ASP_converted','TRF','Total_Taxable_Amount', 'Product_Amount',\n",
    "                        'Planned_Opportunity_Time', 'End_Date_vs_Modified_Date_Difference',\n",
    "                        'Expiry_Date_vs_Modified_Date_Difference', 'Acc_vs_Opp_Creation_Dates']\n",
    "\n",
    "  for column in normalized_columns:\n",
    "    df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "\n",
    "  #Borro columnas que tengan el mismo dato en todas las entradas, o inconsecuentes como el ID / Opportunity_ID\n",
    "  #Algunas columnas borradas son porque pienso que no tienen incidencia, ir viendo.\n",
    "  #TODO: Analizar si el Sales_Contract_No no es que importe el numero en si, sino si tiene\n",
    "  #o no tiene numero de contrato. Por ahora no lo meto como input.\n",
    "  #TODO: Ver el mismo tema con la columna 'Price', la mayoria tiene None u Other\n",
    "  #y solo unos pocos tienen precio numerico. Quiza importe que tenga precio o no tenga,\n",
    "  #o si no tiene precio quiza importe si es None u Other. Por ahora no lo pongo\n",
    "  #como input.\n",
    "  df.drop(columns=['Submitted_for_Approval', 'Last_Activity', 'ASP_(converted)_Currency', \n",
    "                  'Prod_Category_A', 'ID', 'Opportunity_ID', \n",
    "                   'Actual_Delivery_Date'],inplace=True)\n",
    "\n",
    "  #Drop columnas que quiza podamos usar pero por ahora no las uso\n",
    "  df.drop(columns=['Account_Created_Date','Opportunity_Created_Date',\n",
    "                  'Last_Modified_Date',\n",
    "                  'Planned_Delivery_Start_Date','Planned_Delivery_End_Date',\n",
    "                  'Month',\n",
    "                  'Delivery_Year',\n",
    "                  'Price','ASP','Total_Amount_Currency',\n",
    "                  'Total_Amount','Total_Taxable_Amount_Currency', 'Currency',\n",
    "                   'Product_Category_B','Last_Modified_By', 'Account_Owner',\n",
    "                   'Opportunity_Owner','Account_Name','Product_Type',\n",
    "                   'Billing_Country', 'Sales_Contract_No',\n",
    "                   'Product', 'Products','Territory']\n",
    "                   ,inplace=True)\n",
    "\n",
    "\n",
    "  df = df[['Source', 'Opportunity_Type', 'Brand', 'Planned_Opportunity_Time','Total_Taxable_Amount',\n",
    "           'Product_Amount','TRF', 'Bureaucratic_Code','Quote_Type',\n",
    "           'Expiry_Date_vs_Modified_Date_Difference', 'Territory_Is_None', \"Stage\"]]\n",
    "\n",
    "  #Agrego esto para que se pueda usar XGBoost\n",
    "  onehot_columns = ['Source', 'Opportunity_Type', 'Brand', 'Bureaucratic_Code','Quote_Type', 'Territory_Is_None']\n",
    "\n",
    "  onehot_df = df[onehot_columns]\n",
    "  onehot_df = pd.get_dummies(onehot_df, columns = onehot_columns)\n",
    "  df_no_one_hot = df.drop(onehot_columns, axis = 1)\n",
    "  df_one_hot = pd.concat([df_no_one_hot, onehot_df], axis = 1)\n",
    "\n",
    "  #Definimos que tipo de feature es cada columna\n",
    "\n",
    "  #Debemos separar algunos de los registros para armar un set de test propio (no el de la catedra). De esta forma sabremos rapidamente\n",
    "  #si nuestro modelo esta dando resultados optimos o no sin necesidad de estar subiendo el TP a Kaggle constantemente.\n",
    "  #Sin embargo, no queremos usar tantos registros ya que estariamos disminuyendo el set de entrenamiento considerablemente.\n",
    "  #Podemos empezar reservando 2000 registros para el test de prueba y ver que onda. Pasariamos de tener 16 mil a 14 mil \n",
    "  #registros para el set de entrenamiento, no es una perdida importantisima creo en principio, asi que arrancamos con eso.\n",
    "\n",
    "  #Por otro lado, nuestro test de prueba deberia tener un 50 50 de Closed Won y Closed Lost, por lo que no podemos elegir asi nomas\n",
    "  #al azar.\n",
    "\n",
    "  return df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riedel/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:6746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/riedel/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('Train_TP2_Datos_2020-2C.csv')\n",
    "df_train = preprocess_dataframe(df_train)\n",
    "y_train = df_train[\"Stage\"]\n",
    "X_train = df_train.drop(\"Stage\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentamos tirar un valor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=8, random_state=0).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predic = model.predict_proba(X_train)\n",
    "log_loss = metrics.log_loss(y_train, y_predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4295046193521339"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claramente este es un overfitting absurdo, empleo gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riedel/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:6746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/riedel/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('Train_TP2_Datos_2020-2C.csv')\n",
    "df_train = preprocess_dataframe(df_train)\n",
    "columns_order = df_train.columns\n",
    "y = df_train[\"Stage\"]\n",
    "X = df_train.drop(\"Stage\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state= 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[50,100,200], \n",
    "             'max_depth':[None,1,2,3,8,20]}\n",
    "model = RandomForestClassifier()\n",
    "grid = GridSearchCV(model, parameters, scoring = 'neg_log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usamos el grid search de sk learn, supuestamente ya realiza un split interno para enternar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_depth': [None, 1, 2, 3, 8, 20],\n",
       "                         'n_estimators': [50, 100, 200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que resultados tuvimos con cada parametro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL mejor juego de parametros fue\n",
      "\n",
      "{'max_depth': 20, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(\"EL mejor juego de parametros fue\")\n",
    "print()\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EL mejor juego de parametros fue\n",
    "\n",
    "{'max_depth': 20, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34575542566844386"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### vemos el score con el train que queda.\n",
    "y_predict = grid.predict_proba(X_test)\n",
    "log_loss = metrics.log_loss(y_test, y_predict)\n",
    "log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Es un muy buen score, intento afinar un poco mas lo parametros por ese orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[200,250,300,500], \n",
    "             'max_depth':[20,25,30,45,50]}\n",
    "model = RandomForestClassifier()\n",
    "grid = GridSearchCV(model, parameters, scoring = 'neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_depth': [20, 25, 30, 45, 50],\n",
       "                         'n_estimators': [200, 250, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL mejor juego de parametros fue\n",
      "\n",
      "{'max_depth': 20, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "print(\"EL mejor juego de parametros fue\")\n",
    "print()\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL mejor juego de parametros fue\n",
    "\n",
    "{'max_depth': 20, 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34510216890280876"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = grid.predict_proba(X_test)\n",
    "log_loss = metrics.log_loss(y_test, y_predict)\n",
    "log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busco afinar mas todavia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[500,550,600,800], \n",
    "             'max_depth':[15,18,20]}\n",
    "model = RandomForestClassifier()\n",
    "grid = GridSearchCV(model, parameters, scoring = 'neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL mejor juego de parametros fue\n",
      "\n",
      "{'max_depth': 20, 'n_estimators': 550}\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)\n",
    "print(\"EL mejor juego de parametros fue\")\n",
    "print()\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL mejor juego de parametros fue\n",
    "\n",
    "{'max_depth': 20, 'n_estimators': 550}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34566462179359486"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = grid.predict_proba(X_test)\n",
    "log_loss = metrics.log_loss(y_test, y_predict)\n",
    "log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###hago un submit para kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riedel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "frio_test_df = pd.read_csv('Test_TP2_Datos_2020-2C.csv')\n",
    "frio_test_df['Stage'] = 'Closed Won'  #por el preproces de data frame. \n",
    "aux_df = frio_test_df[['Opportunity_ID']] #Esta columna la vuela el preprocesado sino\n",
    "aux_df.drop_duplicates(subset='Opportunity_ID', inplace=True)\n",
    "frio_test_df = preprocess_dataframe(frio_test_df)\n",
    "\n",
    "for col in columns_order:\n",
    "  if not col in frio_test_df.columns:\n",
    "    frio_test_df[col] = 0\n",
    "    \n",
    "frio_test_df = frio_test_df[columns_order]\n",
    "    \n",
    "X_test = frio_test_df.drop(\"Stage\", axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vuelvo a entrenar pero con el dataFrame completo, uso los parametros optimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=550, max_depth=20).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(X_test)\n",
    "aux_df['Target'] = predictions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Opportunity_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>11034</td>\n",
       "      <td>0.012727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>11770</td>\n",
       "      <td>0.632625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>12039</td>\n",
       "      <td>0.457434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Opportunity_ID    Target\n",
       "578            11034  0.012727\n",
       "1552           11770  0.632625\n",
       "2011           12039  0.457434"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_df.to_csv('Out/RandomForest.csv', index=False)\n",
    "aux_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El resultado en kaggle fue de 0.58"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
